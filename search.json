[
  {
    "objectID": "Gadi_setup_guide.html",
    "href": "Gadi_setup_guide.html",
    "title": "Fundamentals of Regression",
    "section": "",
    "text": "Sign up for an NCI account if you don‚Äôt already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join."
  },
  {
    "objectID": "Gadi_setup_guide.html#nci-account-setup",
    "href": "Gadi_setup_guide.html#nci-account-setup",
    "title": "Fundamentals of Regression",
    "section": "",
    "text": "Sign up for an NCI account if you don‚Äôt already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join."
  },
  {
    "objectID": "Gadi_setup_guide.html#nci-australian-research-environment-are",
    "href": "Gadi_setup_guide.html#nci-australian-research-environment-are",
    "title": "Fundamentals of Regression",
    "section": "NCI Australian Research Environment (ARE)",
    "text": "NCI Australian Research Environment (ARE)\n\nConnect to NCI Australian Research Environment.\nBe sure you use your NCI ID (eg, ab1234) for the username and not your email address.\nUnder Featured Apps, find and click the JupterLab: Start a JupyterLab instance option. \nTo Launch a JuptyerLab session, set these resource requirements:\n\n\n\n\n\n\n\nResource\nValue\n\n\n\n\nWalltime (hours)\n5\n\n\nQueue\nnormal\n\n\nCompute Size\nsmall\n\n\nProject\ncd82\n\n\nStorage\nscratch/cd82\n\n\nAdvanced Options‚Ä¶\n\n\n\nModules\npython3/3.9.2\n\n\nPython or Conda virtual environment base\n/scratch/cd82/venv_workshop\n\n\n\nThen click the Launch button.\nThis will take you to your interactive session page you will see that that your JupyterLab session is Queued while ARE is searching for a compute node that will satisfy your requirements.\nOnce found, the page will update with a button that you can click to Open JupyterLab.\nHere is a screenshot of a JupyterLab landing page that should be similar to the one that opens in your web browser after starting the JupyterLab server on either macOS or Windows."
  },
  {
    "objectID": "Gadi_setup_guide.html#transferring-workshop-notebooks",
    "href": "Gadi_setup_guide.html#transferring-workshop-notebooks",
    "title": "Fundamentals of Regression",
    "section": "Transferring workshop notebooks",
    "text": "Transferring workshop notebooks\nWhen you have a Jupyter server running use JupyterLab file navigator to go the folder that has the same name as your username. Then make a new Jupyter notebook by clicking on the ‚ÄúPython 3‚Äù icon under ‚ÄúNotebook‚Äù section and run the following code in a cell:\n!rm -rf /scratch/cd82/$USER/notebooks\n!mkdir -p /scratch/cd82/$USER/notebooks\n!cp /scratch/cd82/regression_notebooks/* /scratch/cd82/$USER/notebooks/\n!ls /scratch/cd82/$USER/notebooks/\nAnd then use the Jupyter file browser to navigate to the directory: /scratch/cd82/$USER/notebooks/ (where $USER is your NCI username)"
  },
  {
    "objectID": "01_introduction_llm.html",
    "href": "01_introduction_llm.html",
    "title": "Installing and checking Python packages",
    "section": "",
    "text": "# !pip install numpy pandas matplotlib openai rank-bm25 requests\n# !pip list | findstr \"numpy pandas matplotlib openai rank-bm25 requests\"",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#background-very-briefly",
    "href": "01_introduction_llm.html#background-very-briefly",
    "title": "Installing and checking Python packages",
    "section": "Background (very briefly)",
    "text": "Background (very briefly)\nAn LLM is a probabilistic model over text. Given a sequence of tokens \\(x_{1:t}\\), it assigns probabilities to the next token \\(x_{t+1}\\). At inference, models sample from a distribution such as \\[p(x_{t+1}=i\\mid x_{1:t}) = \\mathrm{softmax}\\!\\left(\\frac{z_i}{T}\\right),\\] where \\(z_i\\) is the logit for token \\(i\\) and \\(T&gt;0\\) is the temperature. Lower \\(T\\) concentrates probability mass on high-logit tokens (more deterministic), while higher \\(T\\) spreads it out (more diverse). We will demonstrate this behaviour below.",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#environment-setup",
    "href": "01_introduction_llm.html#environment-setup",
    "title": "Installing and checking Python packages",
    "section": "1) Environment Setup",
    "text": "1) Environment Setup\nThis section prepares the Python environment. On QCIF‚Äôs HPC JupyterLab image, the required package (openai) should already be installed. If you‚Äôre running elsewhere and encounter an ImportError, uncomment the %pip install line.\nWhat this cell does: - (Optionally) installs the OpenAI Python client. - Imports the required modules. - Does not make any external calls yet.\n\n# If running outside the provided environment, uncomment the next line:\n# %pip install openai\nimport os\nfrom openai import OpenAI",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#configure-api-connection-groq",
    "href": "01_introduction_llm.html#configure-api-connection-groq",
    "title": "Installing and checking Python packages",
    "section": "2) Configure API Connection (Groq)",
    "text": "2) Configure API Connection (Groq)\nLLM APIs are stateless web services. We‚Äôll configure a client that speaks the OpenAI-compatible protocol, pointing it to Groq‚Äôs base URL.\nWhat you‚Äôll do in this cell: 1. Paste your Groq API key (created at https://console.groq.com). 2. Set the base URL for Groq‚Äôs OpenAI-compatible endpoint. 3. Instantiate the client.\nNotes: - Keep your API key private. In shared workshops, you can paste it, run this cell, and then clear the visible text. - You can also store keys in environment variables or use a .env file if preferred.\n\n# Paste your Groq API key below (between quotes). \n# Note this is not a secure way of entering API key because it is visible to everyone that sees your notebook.  \nos.environ[\"GROQ_API_KEY\"] = \"\"  # &lt;-- replace with your key.\n\n\nmodel = \"llama-3.3-70b-versatile\"\n\n# Groq uses an OpenAI-compatible API surface; we just change the base URL.\nos.environ[\"BASE_URL\"] = \"https://api.groq.com/openai/v1\"\n\n# Create the client\nclient = OpenAI(\n    api_key=os.environ[\"GROQ_API_KEY\"],\n    base_url=os.environ[\"BASE_URL\"],\n)\nprint(\"Groq client initialized!\")\n\nGroq client initialized!",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#first-llm-call-hello-llm-llama-3-70b",
    "href": "01_introduction_llm.html#first-llm-call-hello-llm-llama-3-70b",
    "title": "Installing and checking Python packages",
    "section": "3) First LLM Call ‚Äî ‚ÄúHello LLM‚Äù (Llama-3 70B)",
    "text": "3) First LLM Call ‚Äî ‚ÄúHello LLM‚Äù (Llama-3 70B)\nHere we send a single-turn prompt with minimal scaffolding. The API expects a list of messages, where each message has a role and content.\nRoles: - system: high-level instructions (tone, persona, formatting). - user: your question or instruction. - assistant: the model‚Äôs reply (the API returns this).\nWhat this cell does: - Creates a tiny conversation with system and user messages. - Calls the model llama3-70b-8192 for higher-quality outputs compared to 8B. - Prints the model‚Äôs reply.\nYou can modify the user content and re-run to see different responses.\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Be concise.\"},\n    {\"role\": \"user\", \"content\": \"Explain what a Large Language Model is in two sentences.\"}\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=0 # lower temperature -&gt; more deterministic\n)\n\nprint(response.choices[0].message.content)\n\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, generating human-like text based on the input it receives. LLMs are trained on vast amounts of text data, allowing them to learn patterns and relationships in language, and can be used for tasks such as language translation, text summarization, and conversation generation.\n\n\n\nExamining the response object\n\nfor str in response:\n    print(str)\n\n('id', 'chatcmpl-60e9c394-acc3-4abb-b0e8-a0f03fdf247a')\n('choices', [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, generating human-like text based on the input it receives. LLMs are trained on vast amounts of text data, allowing them to learn patterns and relationships in language, and can be used for tasks such as language translation, text summarization, and conversation generation.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))])\n('created', 1761253879)\n('model', 'llama-3.3-70b-versatile')\n('object', 'chat.completion')\n('service_tier', 'on_demand')\n('system_fingerprint', 'fp_4cfc2deea6')\n('usage', CompletionUsage(completion_tokens=78, prompt_tokens=57, total_tokens=135, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.17950201, prompt_time=0.004199205, completion_time=0.156019343, total_time=0.160218548))\n('usage_breakdown', None)\n('x_groq', {'id': 'req_01k89ejtxpegtvrr9b9v8ep3s6'})\n\n\n\nresponse.choices[0]\n\nChoice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, generating human-like text based on the input it receives. LLMs are trained on vast amounts of text data, allowing them to learn patterns and relationships in language, and can be used for tasks such as language translation, text summarization, and conversation generation.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#understanding-temperature",
    "href": "01_introduction_llm.html#understanding-temperature",
    "title": "Installing and checking Python packages",
    "section": "4) Understanding temperature",
    "text": "4) Understanding temperature\nThe temperature parameter adjusts the randomness of token sampling. Intuitively, the model produces a probability distribution over possible next tokens from its logits \\(z\\). The temperature rescales those logits:\n\\[p_i = \\mathrm{softmax}\\!\\left(\\frac{z_i}{T}\\right) = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}.\\]\n\nLower \\(T\\) (e.g.¬†\\(T=0.2\\)): the distribution is sharper around high-probability tokens, yielding more stable outputs.\nHigher \\(T\\) (e.g.¬†\\(T=0.8\\)): the distribution is flatter, encouraging diversity and sometimes creativity.\n\nWhat this cell does: - Sends the same prompt twice, once with temperature=0.2 and once with temperature=0.8. - Prints both answers so you can compare tone and variability.\n\nprompt = \"Describe the role of LLMs in academic research in one sentence.\"\nfor temp in [0.2, 0.8]:\n    print(\"Temperature:\", temp)\n    for i in range(5):\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=0 # lower temperature -&gt; more deterministic\n        )\n        print(response.choices[0].message.content)\n\nTemperature: 0.2\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate coherent and contextually relevant text, answer questions, and even engage in conversation, mimicking human-like language abilities.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nTemperature: 0.8\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even engage in conversation, making them a powerful tool for various applications such as language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, using complex algorithms to learn patterns and relationships within vast amounts of text data. By training on massive datasets, LLMs can generate human-like text, answer questions, and even converse with users, making them a powerful tool for various applications, including language translation, text summarization, and chatbots.\nA Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language, generating text based on the patterns and structures it has learned from vast amounts of data. LLMs use complex algorithms and massive datasets to predict and create text, enabling applications such as language translation, text summarization, and conversational interfaces.",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#continuous-conversation-keeping-history",
    "href": "01_introduction_llm.html#continuous-conversation-keeping-history",
    "title": "Installing and checking Python packages",
    "section": "5) Continuous Conversation (Keeping History)",
    "text": "5) Continuous Conversation (Keeping History)\nLLM APIs do not keep state between calls. To build a conversation, you keep a list of messages and send the entire recent history each time. We‚Äôll implement a small helper that:\n\nAppends the user‚Äôs message to a global chat_history list.\nCalls the model with that history.\nAppends the assistant‚Äôs reply back into the history.\nReturns the latest reply for display.\n\nWe also keep the temperature low for focused answers. For longer chats, you can cap history to the last k turns to control token usage.\n\ndef chat(user_input, temperature=0.2, max_turns=8):\n    \"\"\"Send one user turn and get a reply, preserving context.\n    - Keeps system prompt + last `max_turns` user/assistant messages.\n    \"\"\"\n    chat_history.append({\"role\": \"user\", \"content\": user_input})\n\n    # Keep only the most recent `max_turns` pairs to control context size\n    system = chat_history[:1]\n    recent = chat_history[-(max_turns*2):] if len(chat_history) &gt; 1 else []\n    window = system + recent\n\n    resp = client.chat.completions.create(\n        model=model,\n        messages=window,\n        temperature=temperature,\n    )\n    reply = resp.choices[0].message.content\n    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n    return reply\n\n\nchat_history = [\n    {\"role\": \"system\", \"content\": \"Tailor your answers for a bioinformatician.\"}\n]\n\nchat(\"What is logistic regression?\")\nchat(\"How does it differ from linear regression?\")\n\nfor ch in chat_history:\n    print(ch)\n\n{'role': 'system', 'content': 'Tailor your answers for a bioinformatician.'}\n{'role': 'user', 'content': 'What is logistic regression?'}\n{'role': 'assistant', 'content': '**Logistic Regression**\\n=======================\\n\\nLogistic regression is a statistical method used for binary classification problems, where the goal is to predict a binary outcome (0/1, yes/no, etc.) based on one or more predictor variables. It is a widely used technique in bioinformatics, particularly in the analysis of high-throughput data, such as gene expression or genomic variation.\\n\\n**Mathematical Formulation**\\n---------------------------\\n\\nLogistic regression models the probability of a positive outcome (e.g., disease presence) using a logistic function, also known as a sigmoid function. The logistic function maps any real-valued number to a value between 0 and 1, which represents the probability of the positive outcome.\\n\\nThe logistic regression model can be formulated as:\\n\\np = 1 / (1 + e^(-z))\\n\\nwhere:\\n\\n* p is the probability of the positive outcome\\n* e is the base of the natural logarithm\\n* z is a linear combination of the predictor variables, weighted by coefficients (Œ≤)\\n\\nz = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n\\nwhere:\\n\\n* Œ≤0 is the intercept or bias term\\n* Œ≤1, Œ≤2, ‚Ä¶, Œ≤n are the coefficients for each predictor variable\\n* x1, x2, ‚Ä¶, xn are the predictor variables\\n\\n**Interpretation of Coefficients**\\n-------------------------------\\n\\nThe coefficients (Œ≤) in logistic regression represent the change in the log-odds of the positive outcome for a one-unit change in the predictor variable, while holding all other variables constant. The odds ratio (OR) can be calculated as e^Œ≤, which represents the multiplicative change in the odds of the positive outcome for a one-unit change in the predictor variable.\\n\\n**Common Applications in Bioinformatics**\\n-----------------------------------------\\n\\n1. **Gene expression analysis**: Logistic regression can be used to identify genes associated with a specific disease or phenotype.\\n2. **Genomic variation analysis**: Logistic regression can be used to identify genetic variants associated with a specific disease or trait.\\n3. **Protein function prediction**: Logistic regression can be used to predict the function of a protein based on its sequence or structural features.\\n4. **Classification of biological samples**: Logistic regression can be used to classify biological samples into different categories (e.g., cancer vs. normal tissue).\\n\\n**Example Code in Python**\\n---------------------------\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create logistic regression model\\nmodel = LogisticRegression()\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\nThis code snippet demonstrates how to use logistic regression to classify biological samples using the scikit-learn library in Python.'}\n{'role': 'user', 'content': 'How does it differ from linear regression?'}\n{'role': 'assistant', 'content': '**Differences between Logistic Regression and Linear Regression**\\n===========================================================\\n\\nLogistic regression and linear regression are both supervised learning algorithms used for regression tasks. However, they differ in their approach, application, and interpretation.\\n\\n**1. Outcome Variable**\\n----------------------\\n\\n* **Linear Regression**: The outcome variable is continuous, such as gene expression levels or protein concentrations.\\n* **Logistic Regression**: The outcome variable is binary (0/1, yes/no, etc.), such as disease presence or absence.\\n\\n**2. Model Formulation**\\n----------------------\\n\\n* **Linear Regression**: The model is formulated as a linear equation, where the outcome variable is a linear combination of the predictor variables.\\n\\t+ y = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n* **Logistic Regression**: The model is formulated as a logistic function, where the probability of the positive outcome is a non-linear function of the predictor variables.\\n\\t+ p = 1 / (1 + e^(-z))\\n\\t+ z = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n\\n**3. Cost Function**\\n-------------------\\n\\n* **Linear Regression**: The cost function is typically mean squared error (MSE) or mean absolute error (MAE).\\n* **Logistic Regression**: The cost function is typically cross-entropy loss or log loss.\\n\\n**4. Interpretation of Coefficients**\\n-----------------------------------\\n\\n* **Linear Regression**: The coefficients represent the change in the outcome variable for a one-unit change in the predictor variable.\\n* **Logistic Regression**: The coefficients represent the change in the log-odds of the positive outcome for a one-unit change in the predictor variable.\\n\\n**5. Assumptions**\\n-----------------\\n\\n* **Linear Regression**: Assumes linearity, independence, homoscedasticity, normality, and no multicollinearity.\\n* **Logistic Regression**: Assumes independence, no multicollinearity, and a binary outcome variable.\\n\\n**Example Code in Python**\\n---------------------------\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create linear regression model\\nlinear_model = LinearRegression()\\n\\n# Train linear model\\nlinear_model.fit(X_train, y_train)\\n\\n# Create logistic regression model\\nlogistic_model = LogisticRegression()\\n\\n# Train logistic model\\nlogistic_model.fit(X_train, (y_train &gt; 0).astype(int))\\n\\n# Evaluate models\\nlinear_accuracy = linear_model.score(X_test, y_test)\\nlogistic_accuracy = logistic_model.score(X_test, (y_test &gt; 0).astype(int))\\nprint(\"Linear Regression Accuracy:\", linear_accuracy)\\nprint(\"Logistic Regression Accuracy:\", logistic_accuracy)\\n```\\n\\nThis code snippet demonstrates how to use both linear regression and logistic regression to model continuous and binary outcome variables, respectively, using the scikit-learn library in Python.\\n\\n**Choosing between Linear Regression and Logistic Regression**\\n---------------------------------------------------------\\n\\n* Use linear regression when the outcome variable is continuous and the relationship between the predictor variables and the outcome variable is linear.\\n* Use logistic regression when the outcome variable is binary and the relationship between the predictor variables and the outcome variable is non-linear.'}\n\n\n\nchat(\"what are some other classification methods?\")\n\nfor ch in chat_history:\n    print(ch)\n\n{'role': 'system', 'content': 'Tailor your answers for a bioinformatician.'}\n{'role': 'user', 'content': 'What is logistic regression?'}\n{'role': 'assistant', 'content': '**Logistic Regression**\\n=======================\\n\\nLogistic regression is a statistical method used for binary classification problems, where the goal is to predict a binary outcome (0/1, yes/no, etc.) based on one or more predictor variables. It is a widely used technique in bioinformatics, particularly in the analysis of high-throughput data, such as gene expression or genomic variation.\\n\\n**Mathematical Formulation**\\n---------------------------\\n\\nLogistic regression models the probability of a positive outcome (e.g., disease presence) using a logistic function, also known as a sigmoid function. The logistic function maps any real-valued number to a value between 0 and 1, which represents the probability of the positive outcome.\\n\\nThe logistic regression model can be formulated as:\\n\\np = 1 / (1 + e^(-z))\\n\\nwhere:\\n\\n* p is the probability of the positive outcome\\n* e is the base of the natural logarithm\\n* z is a linear combination of the predictor variables, weighted by coefficients (Œ≤)\\n\\nz = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n\\nwhere:\\n\\n* Œ≤0 is the intercept or bias term\\n* Œ≤1, Œ≤2, ‚Ä¶, Œ≤n are the coefficients for each predictor variable\\n* x1, x2, ‚Ä¶, xn are the predictor variables\\n\\n**Interpretation of Coefficients**\\n-------------------------------\\n\\nThe coefficients (Œ≤) in logistic regression represent the change in the log-odds of the positive outcome for a one-unit change in the predictor variable, while holding all other variables constant. The odds ratio (OR) can be calculated as e^Œ≤, which represents the multiplicative change in the odds of the positive outcome for a one-unit change in the predictor variable.\\n\\n**Common Applications in Bioinformatics**\\n-----------------------------------------\\n\\n1. **Gene expression analysis**: Logistic regression can be used to identify genes associated with a specific disease or phenotype.\\n2. **Genomic variation analysis**: Logistic regression can be used to identify genetic variants associated with a specific disease or trait.\\n3. **Protein function prediction**: Logistic regression can be used to predict the function of a protein based on its sequence or structural features.\\n4. **Classification of biological samples**: Logistic regression can be used to classify biological samples into different categories (e.g., cancer vs. normal tissue).\\n\\n**Example Code in Python**\\n---------------------------\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create logistic regression model\\nmodel = LogisticRegression()\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\nThis code snippet demonstrates how to use logistic regression to classify biological samples using the scikit-learn library in Python.'}\n{'role': 'user', 'content': 'How does it differ from linear regression?'}\n{'role': 'assistant', 'content': '**Differences between Logistic Regression and Linear Regression**\\n===========================================================\\n\\nLogistic regression and linear regression are both supervised learning algorithms used for regression tasks. However, they differ in their approach, application, and interpretation.\\n\\n**1. Outcome Variable**\\n----------------------\\n\\n* **Linear Regression**: The outcome variable is continuous, such as gene expression levels or protein concentrations.\\n* **Logistic Regression**: The outcome variable is binary (0/1, yes/no, etc.), such as disease presence or absence.\\n\\n**2. Model Formulation**\\n----------------------\\n\\n* **Linear Regression**: The model is formulated as a linear equation, where the outcome variable is a linear combination of the predictor variables.\\n\\t+ y = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n* **Logistic Regression**: The model is formulated as a logistic function, where the probability of the positive outcome is a non-linear function of the predictor variables.\\n\\t+ p = 1 / (1 + e^(-z))\\n\\t+ z = Œ≤0 + Œ≤1 \\\\* x1 + Œ≤2 \\\\* x2 + ‚Ä¶ + Œ≤n \\\\* xn\\n\\n**3. Cost Function**\\n-------------------\\n\\n* **Linear Regression**: The cost function is typically mean squared error (MSE) or mean absolute error (MAE).\\n* **Logistic Regression**: The cost function is typically cross-entropy loss or log loss.\\n\\n**4. Interpretation of Coefficients**\\n-----------------------------------\\n\\n* **Linear Regression**: The coefficients represent the change in the outcome variable for a one-unit change in the predictor variable.\\n* **Logistic Regression**: The coefficients represent the change in the log-odds of the positive outcome for a one-unit change in the predictor variable.\\n\\n**5. Assumptions**\\n-----------------\\n\\n* **Linear Regression**: Assumes linearity, independence, homoscedasticity, normality, and no multicollinearity.\\n* **Logistic Regression**: Assumes independence, no multicollinearity, and a binary outcome variable.\\n\\n**Example Code in Python**\\n---------------------------\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create linear regression model\\nlinear_model = LinearRegression()\\n\\n# Train linear model\\nlinear_model.fit(X_train, y_train)\\n\\n# Create logistic regression model\\nlogistic_model = LogisticRegression()\\n\\n# Train logistic model\\nlogistic_model.fit(X_train, (y_train &gt; 0).astype(int))\\n\\n# Evaluate models\\nlinear_accuracy = linear_model.score(X_test, y_test)\\nlogistic_accuracy = logistic_model.score(X_test, (y_test &gt; 0).astype(int))\\nprint(\"Linear Regression Accuracy:\", linear_accuracy)\\nprint(\"Logistic Regression Accuracy:\", logistic_accuracy)\\n```\\n\\nThis code snippet demonstrates how to use both linear regression and logistic regression to model continuous and binary outcome variables, respectively, using the scikit-learn library in Python.\\n\\n**Choosing between Linear Regression and Logistic Regression**\\n---------------------------------------------------------\\n\\n* Use linear regression when the outcome variable is continuous and the relationship between the predictor variables and the outcome variable is linear.\\n* Use logistic regression when the outcome variable is binary and the relationship between the predictor variables and the outcome variable is non-linear.'}\n{'role': 'user', 'content': 'what are some other classification methods?'}\n{'role': 'assistant', 'content': '**Other Classification Methods**\\n=============================\\n\\nBesides logistic regression, there are several other classification methods used in bioinformatics and machine learning. Here are some of the most common ones:\\n\\n### 1. **Decision Trees**\\n\\n* **Description**: Decision trees are a type of supervised learning algorithm that uses a tree-like model to classify data.\\n* **Application**: Decision trees are useful for handling categorical data and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create decision tree model\\nmodel = DecisionTreeClassifier()\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\n### 2. **Random Forests**\\n\\n* **Description**: Random forests are an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of the model.\\n* **Application**: Random forests are useful for handling high-dimensional data and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create random forest model\\nmodel = RandomForestClassifier(n_estimators=100)\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\n### 3. **Support Vector Machines (SVMs)**\\n\\n* **Description**: SVMs are a type of supervised learning algorithm that uses a hyperplane to separate the data into different classes.\\n* **Application**: SVMs are useful for handling high-dimensional data and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create SVM model\\nmodel = SVC(kernel=\"linear\")\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\n### 4. **K-Nearest Neighbors (KNN)**\\n\\n* **Description**: KNN is a type of supervised learning algorithm that uses the k-nearest neighbors to classify new data points.\\n* **Application**: KNN is useful for handling small datasets and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create KNN model\\nmodel = KNeighborsClassifier(n_neighbors=5)\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\n### 5. **Neural Networks**\\n\\n* **Description**: Neural networks are a type of supervised learning algorithm that uses a network of interconnected nodes (neurons) to classify data.\\n* **Application**: Neural networks are useful for handling complex, high-dimensional data and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create neural network model\\nmodel = MLPClassifier(hidden_layer_sizes=(10, 10))\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\n### 6. **Gradient Boosting**\\n\\n* **Description**: Gradient boosting is an ensemble learning method that combines multiple weak models to create a strong predictive model.\\n* **Application**: Gradient boosting is useful for handling high-dimensional data and can be used for both classification and regression tasks.\\n* **Example Code in Python**:\\n```python\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data\\ndata = pd.read_csv(\"data.csv\")\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"outcome\", axis=1), data[\"outcome\"], test_size=0.2, random_state=42)\\n\\n# Create gradient boosting model\\nmodel = GradientBoostingClassifier(n_estimators=100)\\n\\n# Train model\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate model\\naccuracy = model.score(X_test, y_test)\\nprint(\"Accuracy:\", accuracy)\\n```\\n\\nThese are just a few examples of the many classification methods available in bioinformatics and machine learning. The choice of method depends on the specific problem, dataset, and performance metrics.'}",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#exercise-your-first-prompt",
    "href": "01_introduction_llm.html#exercise-your-first-prompt",
    "title": "Installing and checking Python packages",
    "section": "8) Exercise ‚Äî Your First Prompt",
    "text": "8) Exercise ‚Äî Your First Prompt\nTry your own research-related prompts. A few ideas:\n\nSummarise your current project in one paragraph.\nAsk for three open research questions in your field.\nRequest a draft methods paragraph describing your dataset and analysis steps.\n\nRemember you can tweak temperature to trade off consistency vs creativity.\n\nchat_history = [\n    {\"role\": \"system\", \"content\": \"Tailor your answers for a bioinformatician.\"}\n]\n\n\n# Example: replace with your own question(s)\nprint(chat(\"Summarise the challenges in renewable energy policy research.\"))\n\nAs a bioinformatician, you're likely familiar with complex systems and data-driven approaches. Renewable energy policy research presents several challenges that can be broken down into the following categories:\n\n1. **Integration and Interoperability**: Renewable energy sources, such as solar and wind power, have variable output, making it challenging to integrate them into existing energy grids. This requires advanced forecasting, grid management, and energy storage systems.\n2. **Data Quality and Availability**: High-quality, granular data on energy production, consumption, and grid operations is essential for informed policy decisions. However, data gaps, inconsistencies, and lack of standardization hinder research and policy development.\n3. **Complexity and Uncertainty**: Renewable energy systems involve complex interactions between technological, economic, social, and environmental factors. Uncertainties, such as climate change and policy fluctuations, make it difficult to predict outcomes and develop effective policies.\n4. **Scalability and Spatial Analysis**: Renewable energy deployment requires consideration of spatial factors, such as land use, resource availability, and infrastructure. Scalability issues arise when trying to balance local, regional, and global energy demands with available resources.\n5. **Stakeholder Engagement and Social Acceptance**: Effective policy development requires engagement with diverse stakeholders, including communities, industries, and governments. Social acceptance of renewable energy technologies and infrastructure can be a significant challenge.\n6. **Economic and Financial Analysis**: Renewable energy policies often involve economic incentives, subsidies, and investments. Accurate economic and financial analysis is necessary to evaluate policy effectiveness, but this can be complicated by factors like technology costs, market volatility, and externalities.\n7. **Policy Frameworks and Governance**: Renewable energy policies must navigate existing regulatory frameworks, which can be inadequate or inconsistent. Effective governance structures, including international cooperation and national policies, are essential for supporting the transition to renewable energy.\n8. **Technological Innovation and Deployment**: The rapid evolution of renewable energy technologies creates challenges for policy development, as new technologies and innovations can disrupt existing markets and infrastructure.\n9. **Energy Justice and Equity**: Renewable energy policies must address issues of energy access, affordability, and equity, particularly for marginalized communities. This requires careful consideration of social and environmental impacts.\n10. **Long-term Planning and Scenario Development**: Renewable energy policy research requires long-term planning and scenario development to anticipate future energy demands, technology advancements, and potential risks.\n\nTo address these challenges, researchers can employ a range of bioinformatics-inspired approaches, such as:\n\n* Data integration and analytics\n* Machine learning and predictive modeling\n* Network analysis and simulation\n* Spatial analysis and geospatial modeling\n* Stakeholder engagement and participatory modeling\n* Scenario planning and uncertainty analysis\n\nBy leveraging these approaches, researchers can develop more effective renewable energy policies that balance technological, economic, social, and environmental considerations.",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01_introduction_llm.html#wrap-up-next-steps",
    "href": "01_introduction_llm.html#wrap-up-next-steps",
    "title": "Installing and checking Python packages",
    "section": "9) Wrap-Up & Next Steps",
    "text": "9) Wrap-Up & Next Steps\n\nYou configured an OpenAI-compatible client to talk to Groq.\nYou sent your first prompts using Llama-3 70B and explored the impact of temperature.\nYou kept conversation state locally in a Python list and learned how to save it.\n\nIn the next notebook, we‚Äôll connect to a scholarly API to fetch abstracts and practice literature summarisation and structured extraction.\nKey terms: tokens, temperature, logits, softmax, stateless API, chat history.",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "00_setup_guide.html",
    "href": "00_setup_guide.html",
    "title": "Large Language Models for Research - QCIF",
    "section": "",
    "text": "Author: Moji Ghadimi https://www.linkedin.com/in/moji-ghadimi/"
  },
  {
    "objectID": "00_setup_guide.html#install-python-recommended-version-3.10-or-newer",
    "href": "00_setup_guide.html#install-python-recommended-version-3.10-or-newer",
    "title": "Large Language Models for Research - QCIF",
    "section": "üêç 1. Install Python (Recommended: Version 3.10 or newer)",
    "text": "üêç 1. Install Python (Recommended: Version 3.10 or newer)\n\nWindows\n\nGo to the official Python website: https://www.python.org/downloads/\nDownload the latest Python 3.x installer.\nRun the installer and check the box that says:\n\n‚úÖ Add Python to PATH\n\nChoose Install Now and follow the prompts.\n\nTo verify installation, open Command Prompt (cmd) and type:\npython --version\nYou should see something like:\nPython 3.11.6\n\n\n\nmacOS / Linux\nMost systems come with Python preinstalled. You can check your version:\npython3 --version\nIf Python is not installed, install it using your system package manager:\nmacOS (Homebrew):\nbrew install python\nUbuntu / Debian:\nsudo apt update && sudo apt install python3 python3-pip -y"
  },
  {
    "objectID": "00_setup_guide.html#install-jupyterlab",
    "href": "00_setup_guide.html#install-jupyterlab",
    "title": "Large Language Models for Research - QCIF",
    "section": "üì¶ 2. Install jupyterlab",
    "text": "üì¶ 2. Install jupyterlab\nOnce Python is installed, you can install packages using pip (Python‚Äôs package manager).\nRun this command in your terminal or command prompt:\npip install jupyterlab"
  },
  {
    "objectID": "00_setup_guide.html#launch-jupyterlab",
    "href": "00_setup_guide.html#launch-jupyterlab",
    "title": "Large Language Models for Research - QCIF",
    "section": "üß† 3. Launch JupyterLab",
    "text": "üß† 3. Launch JupyterLab\nOpen terminal or command prompt and navigate to the folder containing your workshop notebooks (or navigate then open), then run:\njupyter lab\nThis will open a new tab in your default web browser with the JupyterLab interface.\nYou can then click on any notebook to open it."
  },
  {
    "objectID": "00_setup_guide.html#running-on-hpc",
    "href": "00_setup_guide.html#running-on-hpc",
    "title": "Large Language Models for Research - QCIF",
    "section": "‚öôÔ∏è 4. Running on HPC",
    "text": "‚öôÔ∏è 4. Running on HPC\nIf you‚Äôre using a hosted JupyterLab instance on HPC infrastructure, Python and most dependencies are already installed. You typically only need to:\n\nOpen the web-based JupyterLab interface.\nLoad the correct Python environment (via module or dropdown).\nStart a notebook directly."
  },
  {
    "objectID": "00_setup_guide.html#install-required-python-packages",
    "href": "00_setup_guide.html#install-required-python-packages",
    "title": "Large Language Models for Research - QCIF",
    "section": "üì¶ 5. Install Required Python Packages",
    "text": "üì¶ 5. Install Required Python Packages\nOpen a notebook and in a cell run the command below to install packages:\n!pip install pandas matplotlib openai rank-bm25 requests \nThis will install:\n\nnumpy ‚Äì for handling arrays.\npandas ‚Äì for handling data.\nmatplotlib ‚Äì for plotting.\nopenai ‚Äì to connect to Groq or OpenAI-compatible APIs.\nrank-bm25 ‚Äì for retrieval-based tasks.\nrequests ‚Äì for fetching data from APIs.\njupyterlab ‚Äì for running notebooks interactively.\n\nTo verify installation:\npip list | findstr \"numpy pandas matplotlib openai rank-bm25 requests\"\nYou should see all packages listed."
  },
  {
    "objectID": "00_setup_guide.html#summary",
    "href": "00_setup_guide.html#summary",
    "title": "Large Language Models for Research - QCIF",
    "section": "‚úÖ Summary",
    "text": "‚úÖ Summary\n\nInstall Python ‚â• 3.10\nUse pip to install required packages\nRun jupyter lab\nVerify everything by opening the example notebooks"
  },
  {
    "objectID": "02_theory_and_concepts.html",
    "href": "02_theory_and_concepts.html",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "",
    "text": "This notebook introduces the conceptual foundations of Large Language Models (LLMs) in research contexts. Before working with APIs, it‚Äôs crucial to understand what LLMs are, what they can and cannot do, and how to use them responsibly.\nWe will cover: - What LLMs are and how they work - Strengths and weaknesses in research use - Basics of prompt engineering - Why hallucination occurs and how to detect it - Benefits and risks across use cases - Concept of Agentic AI - Running LLMs via API vs locally"
  },
  {
    "objectID": "02_theory_and_concepts.html#what-is-a-large-language-model-llm",
    "href": "02_theory_and_concepts.html#what-is-a-large-language-model-llm",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "1. What Is a Large Language Model (LLM)?",
    "text": "1. What Is a Large Language Model (LLM)?\nAn LLM is a probabilistic text generator. It predicts the next word in a sequence based on all the previous words.\nFormally, it learns to estimate:\n\\[p(x_{t+1} \\mid x_{1:t}) = \\text{softmax}\\left(\\frac{z_i}{T}\\right)\\]\nwhere \\(z_i\\) are logits (raw scores), and \\(T\\) is temperature controlling randomness.\nKey ideas: - The model is not reasoning; it‚Äôs pattern matching. - It is trained on billions of text samples. - It does not know whether statements are true.\nüí° Analogy: LLMs complete text the way autocomplete finishes a sentence ‚Äî just on a massive scale."
  },
  {
    "objectID": "02_theory_and_concepts.html#what-llms-are-good-for-and-what-they-are-not",
    "href": "02_theory_and_concepts.html#what-llms-are-good-for-and-what-they-are-not",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "2. What LLMs Are Good For ‚Äî and What They Are Not",
    "text": "2. What LLMs Are Good For ‚Äî and What They Are Not\n\n\n\n‚úÖ Good At\n‚ö†Ô∏è Not Good At\n\n\n\n\nSummarising academic text\nProducing verified facts\n\n\nParaphrasing and rewriting\nMathematical proofs or derivations\n\n\nExplaining code or methods\nStatistical inference without data\n\n\nGenerating boilerplate writing\nHandling private or sensitive data\n\n\nBrainstorming research ideas\nActing as a source of truth\n\n\n\nThink of an LLM as an assistant, not a co-author."
  },
  {
    "objectID": "02_theory_and_concepts.html#prompt-engineering-essentials",
    "href": "02_theory_and_concepts.html#prompt-engineering-essentials",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "3. Prompt Engineering Essentials",
    "text": "3. Prompt Engineering Essentials\nPrompt engineering means crafting the input to shape the model‚Äôs response.\n\nExample\n‚ùå Bad: Explain machine learning.\n‚úÖ Better: In three sentences, explain machine learning to a biology PhD student unfamiliar with computer science.\n\n\nBest Practices\n\nBe specific about role, audience, and format.\nUse system messages to set tone or constraints.\nBreak complex queries into smaller sub-prompts.\nAsk for structured output (e.g., JSON, tables).\n\nLet‚Äôs illustrate how different prompt structures might be interpreted.\n\n# Demonstration: Prompt variety examples (no API calls)\n# This cell just prints examples and explanations of what makes prompts effective.\n\nexample_prompts = [\n    (\"Explain AI.\", \"Too vague ‚Äî model may produce generic output.\"),\n    (\"Explain Artificial Intelligence in two sentences for an interdisciplinary audience.\", \"Better ‚Äî specifies length and audience.\"),\n    (\"As a data scientist, summarise Artificial Intelligence focusing on statistical learning methods.\", \"Excellent ‚Äî adds role and context, leading to relevant focus.\")\n]\n\nfor text, comment in example_prompts:\n    print(f\"Prompt: {text}\\nComment: {comment}\\n{'-'*70}\")\n\nPrompt: Explain AI.\nComment: Too vague ‚Äî model may produce generic output.\n----------------------------------------------------------------------\nPrompt: Explain Artificial Intelligence in two sentences for an interdisciplinary audience.\nComment: Better ‚Äî specifies length and audience.\n----------------------------------------------------------------------\nPrompt: As a data scientist, summarise Artificial Intelligence focusing on statistical learning methods.\nComment: Excellent ‚Äî adds role and context, leading to relevant focus.\n----------------------------------------------------------------------\n\n\n\n\nReflection\n\nHow would you phrase prompts for your research area?\nWhat happens if your question is ambiguous or underspecified?\nHow might prompt reproducibility affect research transparency?"
  },
  {
    "objectID": "02_theory_and_concepts.html#why-hallucination-happens",
    "href": "02_theory_and_concepts.html#why-hallucination-happens",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "4. Why Hallucination Happens",
    "text": "4. Why Hallucination Happens\nHallucination is when the model produces false but plausible information.\nCauses: 1. Models optimise for fluency, not truth. 2. They lack external knowledge verification. 3. They use patterns, not evidence.\nMitigations: - Use grounded prompts (e.g., with text context or retrieval). - Ask for sources and check them. - Rephrase prompts to encourage uncertainty awareness (e.g., ‚ÄúIf unsure, say so.‚Äù)\n\n# Simulating hallucination detection with a fabricated response\n# This code checks whether a response contains uncertainty words.\n\nresponse = \"Dr. Jane Quantum won the Nobel Prize in Quantum Psychology in 2024.\"\n\nuncertainty_markers = [\"might\", \"possibly\", \"may\", \"perhaps\", \"uncertain\"]\nuncertain = any(word in response.lower() for word in uncertainty_markers)\n\nif uncertain:\n    print(\"‚úÖ The statement expresses uncertainty.\")\nelse:\n    print(\"‚ö†Ô∏è This response shows *overconfidence* ‚Äî likely hallucination.\")\n\n‚ö†Ô∏è This response shows *overconfidence* ‚Äî likely hallucination."
  },
  {
    "objectID": "02_theory_and_concepts.html#benefits-and-risks-in-research",
    "href": "02_theory_and_concepts.html#benefits-and-risks-in-research",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "5. Benefits and Risks in Research",
    "text": "5. Benefits and Risks in Research\n\n\n\n\n\n\n\n\nApplication\nBenefit\nRisk / Limitation\n\n\n\n\nLiterature summarisation\nSaves time, finds patterns\nHallucinated facts or missing nuance\n\n\nCoding help\nFaster prototyping\nWrong syntax or unsafe imports\n\n\nAcademic writing\nBetter grammar, flow\nStyle drift, plagiarism concerns\n\n\nBrainstorming ideas\nExpands creativity\nMay output unverified claims\n\n\nData cleaning\nQuick suggestions\nMay fabricate column names\n\n\n\nüß≠ Always cross-check LLM-generated outputs before citing or integrating into research."
  },
  {
    "objectID": "02_theory_and_concepts.html#agentic-ai-concept-only",
    "href": "02_theory_and_concepts.html#agentic-ai-concept-only",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "6. Agentic AI (Concept Only)",
    "text": "6. Agentic AI (Concept Only)\nAgentic AI refers to models that can take initiative ‚Äî plan actions, call tools, and iteratively refine results.\nExamples: AutoGPT, LangChain Agents, ChatGPT with code or browsing.\nThey combine: - Planning (deciding what to do next) - Memory (recalling previous steps) - Tool use (e.g., running Python or querying databases)\n\nWhy it matters\n\nMoves from passive Q&A to autonomous workflows.\nRaises questions of accountability and control.\n\nüß© The RAG (Retrieval-Augmented Generation) notebook later in this series is a small step toward agentic systems."
  },
  {
    "objectID": "02_theory_and_concepts.html#running-llms-via-api-vs-locally",
    "href": "02_theory_and_concepts.html#running-llms-via-api-vs-locally",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "7. Running LLMs via API vs Locally",
    "text": "7. Running LLMs via API vs Locally\n\nComparison\n\n\n\n\n\n\n\n\nApproach\nPros\nCons\n\n\n\n\nAPI (Groq, OpenAI)\nNo setup, scalable, always updated\nRequires internet, API cost\n\n\nLocal (Hugging Face, vLLM)\nFull control, offline\nNeeds high VRAM, complex setup\n\n\n\nBelow we estimate how much GPU memory is needed to run different models locally.\n\n# Estimate GPU VRAM requirement for hosting models locally\n# Rule of thumb: 1 billion parameters ‚âà 2 GB VRAM (16-bit precision)\n\ndef estimate_vram(params_billion, precision_bits=16):\n    \"\"\"Estimate GPU memory needed for model parameters.\"\"\"\n    bytes_per_param = precision_bits / 8\n    total_gb = (params_billion * 1e9 * bytes_per_param) / (1e9 * 1.024)\n    return round(total_gb, 1)\n\nmodels = {\"Llama-3 8B\": 8, \"Llama-3 70B\": 70, \"Mistral 7B\": 7}\n\nprint(\"Approximate VRAM needed (16-bit precision):\\n\")\nfor model, size in models.items():\n    print(f\"{model:15s}: {estimate_vram(size)} GB\")\n\nApproximate VRAM needed (16-bit precision):\n\nLlama-3 8B     : 15.6 GB\nLlama-3 70B    : 136.7 GB\nMistral 7B     : 13.7 GB\n\n\n‚û°Ô∏è A 70B model would require well over 140 GB of GPU VRAM, so APIs are currently the most practical solution for most researchers."
  },
  {
    "objectID": "02_theory_and_concepts.html#pros-and-cons-summary",
    "href": "02_theory_and_concepts.html#pros-and-cons-summary",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "8. Pros and Cons Summary",
    "text": "8. Pros and Cons Summary\n\n\n\nAspect\nPros\nCons\n\n\n\n\nEase of use\nMinimal setup\nReliance on vendor uptime\n\n\nCost\nFree/cheap for small workloads\nExpensive at scale\n\n\nReproducibility\nControlled APIs\nModel updates may change outputs\n\n\nEthics\nAccessible to all\nPrivacy and bias concerns\n\n\n\nBalance practicality with reproducibility ‚Äî document all model versions and API calls when publishing results."
  },
  {
    "objectID": "02_theory_and_concepts.html#reflection-1",
    "href": "02_theory_and_concepts.html#reflection-1",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "9. Reflection",
    "text": "9. Reflection\nQuestions for you: 1. Which tasks in your workflow could LLMs assist with? 2. What risks might arise from automation in your field? 3. How can you document LLM involvement transparently in your papers?\nWrite your reflections below as markdown cells."
  },
  {
    "objectID": "02_theory_and_concepts.html#summary",
    "href": "02_theory_and_concepts.html#summary",
    "title": "Theory and Concepts: Understanding LLMs Before You Trust Them",
    "section": "‚úÖ Summary",
    "text": "‚úÖ Summary\n\nLLMs predict words, not truths ‚Äî verification is essential.\nPrompt engineering is key to consistent behaviour.\nHallucinations are unavoidable but detectable.\nAgentic AI is emerging; RAG is its foundation.\nAPIs simplify use; local models give control but need hardware.\n\n‚û°Ô∏è Next, we‚Äôll use these concepts in practice to perform literature summarisation using Groq + Llama-3 70B."
  }
]