{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Theory and Concepts: Understanding LLMs Before You Trust Them\n",
        "\n",
        "This notebook introduces the conceptual foundations of Large Language Models (LLMs) in research contexts. Before working with APIs, it's crucial to understand what LLMs are, what they can and cannot do, and how to use them responsibly.\n",
        "\n",
        "We will cover:\n",
        "- What LLMs are and how they work\n",
        "- Strengths and weaknesses in research use\n",
        "- Basics of **prompt engineering**\n",
        "- Why **hallucination** occurs and how to detect it\n",
        "- Benefits and risks across use cases\n",
        "- Concept of **Agentic AI**\n",
        "- Running LLMs via **API vs locally**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What Is a Large Language Model (LLM)?\n",
        "\n",
        "An LLM is a **probabilistic text generator**. It predicts the next word in a sequence based on all the previous words.\n",
        "\n",
        "Formally, it learns to estimate:\n",
        "\n",
        "$$p(x_{t+1} \\mid x_{1:t}) = \\text{softmax}\\left(\\frac{z_i}{T}\\right)$$\n",
        "\n",
        "where $z_i$ are logits (raw scores), and $T$ is temperature controlling randomness.\n",
        "\n",
        "**Key ideas:**\n",
        "- The model is not reasoning; it's *pattern matching*.\n",
        "- It is trained on billions of text samples.\n",
        "- It does not know whether statements are *true*.\n",
        "\n",
        "üí° **Analogy:** LLMs complete text the way autocomplete finishes a sentence ‚Äî just on a massive scale."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b1e7db",
      "metadata": {},
      "source": [
        "## 2. What LLMs Are Good For ‚Äî and What They Are Not\n",
        "\n",
        "| ‚úÖ Good At | ‚ö†Ô∏è Not Good At |\n",
        "|-------------|---------------|\n",
        "| Summarising academic text | Producing verified facts |\n",
        "| Paraphrasing and rewriting | Mathematical proofs or derivations |\n",
        "| Explaining code or methods | Statistical inference without data |\n",
        "| Generating boilerplate writing | Handling private or sensitive data |\n",
        "| Brainstorming research ideas | Acting as a source of truth |\n",
        "\n",
        "Think of an LLM as an *assistant*, not a *co-author*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt Engineering Essentials\n",
        "\n",
        "Prompt engineering means **crafting the input** to shape the model's response.\n",
        "\n",
        "### Example\n",
        "```\n",
        "‚ùå Bad: Explain machine learning.\n",
        "‚úÖ Better: In three sentences, explain machine learning to a biology PhD student unfamiliar with computer science.\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "- Be **specific** about role, audience, and format.\n",
        "- Use **system messages** to set tone or constraints.\n",
        "- Break complex queries into smaller sub-prompts.\n",
        "- Ask for **structured output** (e.g., JSON, tables).\n",
        "\n",
        "Let‚Äôs illustrate how different prompt structures might be interpreted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b45bc63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain AI.\n",
            "Comment: Too vague ‚Äî model may produce generic output.\n",
            "----------------------------------------------------------------------\n",
            "Prompt: Explain Artificial Intelligence in two sentences for an interdisciplinary audience.\n",
            "Comment: Better ‚Äî specifies length and audience.\n",
            "----------------------------------------------------------------------\n",
            "Prompt: As a data scientist, summarise Artificial Intelligence focusing on statistical learning methods.\n",
            "Comment: Excellent ‚Äî adds role and context, leading to relevant focus.\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Demonstration: Prompt variety examples (no API calls)\n",
        "# This cell just prints examples and explanations of what makes prompts effective.\n",
        "\n",
        "example_prompts = [\n",
        "    (\"Explain AI.\", \"Too vague ‚Äî model may produce generic output.\"),\n",
        "    (\"Explain Artificial Intelligence in two sentences for an interdisciplinary audience.\", \"Better ‚Äî specifies length and audience.\"),\n",
        "    (\"As a data scientist, summarise Artificial Intelligence focusing on statistical learning methods.\", \"Excellent ‚Äî adds role and context, leading to relevant focus.\")\n",
        "]\n",
        "\n",
        "for text, comment in example_prompts:\n",
        "    print(f\"Prompt: {text}\\nComment: {comment}\\n{'-'*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354067e4",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- How would you phrase prompts for your research area?\n",
        "- What happens if your question is ambiguous or underspecified?\n",
        "- How might prompt reproducibility affect research transparency?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ba41c",
      "metadata": {},
      "source": [
        "## 4. Why Hallucination Happens\n",
        "\n",
        "**Hallucination** is when the model produces false but plausible information.\n",
        "\n",
        "**Causes:**\n",
        "1. Models optimise for *fluency*, not *truth*.\n",
        "2. They lack external knowledge verification.\n",
        "3. They use patterns, not evidence.\n",
        "\n",
        "**Mitigations:**\n",
        "- Use grounded prompts (e.g., with text context or retrieval).\n",
        "- Ask for *sources* and check them.\n",
        "- Rephrase prompts to encourage uncertainty awareness (e.g., ‚ÄúIf unsure, say so.‚Äù)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8bc4dfe6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è This response shows *overconfidence* ‚Äî likely hallucination.\n"
          ]
        }
      ],
      "source": [
        "# Simulating hallucination detection with a fabricated response\n",
        "# This code checks whether a response contains uncertainty words.\n",
        "\n",
        "response = \"Dr. Jane Quantum won the Nobel Prize in Quantum Psychology in 2024.\"\n",
        "\n",
        "uncertainty_markers = [\"might\", \"possibly\", \"may\", \"perhaps\", \"uncertain\"]\n",
        "uncertain = any(word in response.lower() for word in uncertainty_markers)\n",
        "\n",
        "if uncertain:\n",
        "    print(\"‚úÖ The statement expresses uncertainty.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è This response shows *overconfidence* ‚Äî likely hallucination.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec89ae5e",
      "metadata": {},
      "source": [
        "## 5. Benefits and Risks in Research\n",
        "\n",
        "| Application | Benefit | Risk / Limitation |\n",
        "|--------------|----------|------------------|\n",
        "| Literature summarisation | Saves time, finds patterns | Hallucinated facts or missing nuance |\n",
        "| Coding help | Faster prototyping | Wrong syntax or unsafe imports |\n",
        "| Academic writing | Better grammar, flow | Style drift, plagiarism concerns |\n",
        "| Brainstorming ideas | Expands creativity | May output unverified claims |\n",
        "| Data cleaning | Quick suggestions | May fabricate column names |\n",
        "\n",
        "üß≠ Always cross-check LLM-generated outputs before citing or integrating into research."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc06c37b",
      "metadata": {},
      "source": [
        "## 6. Agentic AI (Concept Only)\n",
        "\n",
        "**Agentic AI** refers to models that can take *initiative* ‚Äî plan actions, call tools, and iteratively refine results.\n",
        "\n",
        "**Examples:** AutoGPT, LangChain Agents, ChatGPT with code or browsing.\n",
        "\n",
        "They combine:\n",
        "- **Planning** (deciding what to do next)\n",
        "- **Memory** (recalling previous steps)\n",
        "- **Tool use** (e.g., running Python or querying databases)\n",
        "\n",
        "### Why it matters\n",
        "- Moves from passive Q&A to *autonomous workflows*.\n",
        "- Raises questions of **accountability** and **control**.\n",
        "\n",
        "üß© The RAG (Retrieval-Augmented Generation) notebook later in this series is a *small step* toward agentic systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6702a3e9",
      "metadata": {},
      "source": [
        "## 7. Running LLMs via API vs Locally\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|-----------|------|------|\n",
        "| **API (Groq, OpenAI)** | No setup, scalable, always updated | Requires internet, API cost, data privacy concerns |\n",
        "| **Local (Hugging Face, vLLM)** | Full control, offline | Needs high VRAM, complex setup |\n",
        "\n",
        "Below we estimate how much GPU memory is needed to run different models locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f3bafeb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approximate VRAM needed (16-bit precision):\n",
            "\n",
            "Llama-3 8B     : 15.6 GB\n",
            "Llama-3 70B    : 136.7 GB\n",
            "Mistral 7B     : 13.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Estimate GPU VRAM requirement for hosting models locally\n",
        "# Rule of thumb: 1 billion parameters ‚âà 2 GB VRAM (16-bit precision)\n",
        "\n",
        "def estimate_vram(params_billion, precision_bits=16):\n",
        "    \"\"\"Estimate GPU memory needed for model parameters.\"\"\"\n",
        "    bytes_per_param = precision_bits / 8\n",
        "    total_gb = (params_billion * 1e9 * bytes_per_param) / (1e9 * 1.024)\n",
        "    return round(total_gb, 1)\n",
        "\n",
        "models = {\"Llama-3 8B\": 8, \"Llama-3 70B\": 70, \"Mistral 7B\": 7}\n",
        "\n",
        "print(\"Approximate VRAM needed (16-bit precision):\\n\")\n",
        "for model, size in models.items():\n",
        "    print(f\"{model:15s}: {estimate_vram(size)} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚û°Ô∏è A 70B model would require well over **140 GB of GPU VRAM**, so APIs are currently the most practical solution for most researchers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pros and Cons Summary of using API\n",
        "\n",
        "| Aspect | Pros | Cons |\n",
        "|---------|------|------|\n",
        "| Ease of use | Minimal setup | Reliance on vendor uptime |\n",
        "| Cost | Free/cheap for small workloads | Expensive at scale |\n",
        "| Reproducibility | Controlled APIs | Model updates may change outputs |\n",
        "| Ethics | Accessible to all | Privacy and bias concerns |\n",
        "\n",
        "Balance practicality with reproducibility ‚Äî document all model versions and API calls when publishing results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Reflection\n",
        "\n",
        "**Questions for you:**\n",
        "1. Which tasks in your workflow could LLMs assist with?\n",
        "2. What risks might arise from automation in your field?\n",
        "3. How can you document LLM involvement transparently in your papers?\n",
        "\n",
        "Write your reflections below as markdown cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "- LLMs predict words, not truths ‚Äî verification is essential.\n",
        "- Prompt engineering is key to consistent behaviour.\n",
        "- Hallucinations are unavoidable but detectable.\n",
        "- Agentic AI is emerging; RAG is its foundation.\n",
        "- APIs simplify use; local models give control but need hardware.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
