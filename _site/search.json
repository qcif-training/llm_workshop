[
  {
    "objectID": "07_tree_regression.html",
    "href": "07_tree_regression.html",
    "title": "Decision Tree Regression",
    "section": "",
    "text": "In this lesson we train a DecisionTreeRegressor on the Diabetes dataset.\nWe will cover splitting criteria, key hyperparameters, pruning via validation, and interpretation (feature importance and partial dependence).\nLearning objectives - Explain how regression trees split to reduce squared error. - Tune depth/leaf hyperparameters with cross‑validation (HPC‑friendly). - Diagnose overfitting vs underfitting with plots. - Interpret models using feature importance and partial dependence."
  },
  {
    "objectID": "07_tree_regression.html#how-regression-trees-work-theory",
    "href": "07_tree_regression.html#how-regression-trees-work-theory",
    "title": "Decision Tree Regression",
    "section": "How Regression Trees Work (Theory)",
    "text": "How Regression Trees Work (Theory)\nAt each node a regression tree chooses a feature \\(j\\) and threshold \\(t\\) to split the data into left/right partitions and minimize the impurity (here, total squared error): \\[\n\\text{SSE}(\\mathcal{S}) = \\sum_{(x_i,y_i)\\in \\mathcal{S}} (y_i - \\bar{y}_{\\mathcal{S}})^2.\n\\] The best split \\((j,t)\\) maximizes the reduction in SSE: \\[\n\\Delta = \\text{SSE}(\\mathcal{S}) - \\big[\\text{SSE}(\\mathcal{S}_L) + \\text{SSE}(\\mathcal{S}_R)\\big].\n\\]\nPredictions in a leaf are the mean of training targets in that leaf.\nKey hyperparameters control complexity: - max_depth: maximum tree depth (smaller \\(\\\\Rightarrow\\) simpler model). - min_samples_leaf: minimum samples per leaf (larger \\(\\\\Rightarrow\\) smoother predictions). - min_samples_split: minimum samples to split an internal node. - ccp_alpha: cost‑complexity pruning strength.\n\n# Load Diabetes dataset\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nX = df.drop(columns=['disease_progression'])\ny = df['disease_progression']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n\n\n\n\n\n\n\n\n# Baseline DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\ntree = DecisionTreeRegressor(random_state=42)\ntree.fit(X_train, y_train)\n\ny_pred_train = tree.predict(X_train)\ny_pred_test  = tree.predict(X_test)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test  = np.sqrt(mean_squared_error(y_test, y_pred_test))\nmae_train  = mean_absolute_error(y_train, y_pred_train)\nmae_test   = mean_absolute_error(y_test, y_pred_test)\nr2_train   = r2_score(y_train, y_pred_train)\nr2_test    = r2_score(y_test, y_pred_test)\n\nprint(f\"Train: RMSE={rmse_train:.2f}  MAE={mae_train:.2f}  R^2={r2_train:.3f}\")\nprint(f\"Test : RMSE={rmse_test:.2f}   MAE={mae_test:.2f}   R^2={r2_test:.3f}\")\n\nTrain: RMSE=0.00  MAE=0.00  R^2=1.000\nTest : RMSE=70.55   MAE=54.53   R^2=0.061\n\n\n\n# Diagnostics: predicted vs actual and residuals\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Predicted vs Actual\nplt.figure(figsize=(5,5))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nlo = min(y_test.min(), y_pred_test.min())\nhi = max(y_test.max(), y_pred_test.max())\nplt.plot([lo, hi], [lo, hi])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Predicted vs Actual (Test)\")\nplt.show()\n\n# Residuals vs Predicted\nresiduals = y_test - y_pred_test\nplt.figure(figsize=(6,4))\nplt.scatter(y_pred_test, residuals, alpha=0.7)\nplt.axhline(0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residual (y - ŷ)\")\nplt.title(\"Residuals vs Predicted (Test)\")\nplt.show()"
  },
  {
    "objectID": "07_tree_regression.html#crossvalidation-pruning",
    "href": "07_tree_regression.html#crossvalidation-pruning",
    "title": "Decision Tree Regression",
    "section": "Cross‑Validation & Pruning",
    "text": "Cross‑Validation & Pruning\nWe tune max_depth, min_samples_leaf, and ccp_alpha using \\(k\\)‑fold CV.\nOn some HPC setups, default parallelism can fail; we wrap with the threading backend (or use n_jobs=1).\n\n# Grid search for tree hyperparameters (HPC-friendly)\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\n\nparam_grid = {\n    'max_depth': [None, 3, 4, 5, 6, 8],\n    'min_samples_leaf': [1, 3, 5, 10],\n    'ccp_alpha': [0.0, 0.001, 0.005, 0.01]\n}\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\ntree = DecisionTreeRegressor(random_state=42)\n\ntry:\n    from joblib import parallel_backend\n    with parallel_backend('threading'):\n        gsv = GridSearchCV(tree, param_grid, scoring='r2', cv=cv, n_jobs=-1, refit=True)\n        gsv.fit(X_train, y_train)\nexcept Exception:\n    gsv = GridSearchCV(tree, param_grid, scoring='r2', cv=cv, n_jobs=1, refit=True)\n    gsv.fit(X_train, y_train)\n\nprint(\"Best params:\", gsv.best_params_)\nprint(\"Best CV R^2:\", round(gsv.best_score_, 3))\n\nbest_tree = gsv.best_estimator_\ny_pred_test_best = best_tree.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_test_best))\nmae  = mean_absolute_error(y_test, y_pred_test_best)\nr2   = r2_score(y_test, y_pred_test_best)\n\nprint(f\"Test metrics (tuned): RMSE={rmse:.2f}  MAE={mae:.2f}  R^2={r2:.3f}\")\n\nBest params: {'ccp_alpha': 0.0, 'max_depth': 4, 'min_samples_leaf': 3}\nBest CV R^2: 0.284\nTest metrics (tuned): RMSE=58.50  MAE=46.47  R^2=0.354"
  },
  {
    "objectID": "07_tree_regression.html#visualizing-the-tree",
    "href": "07_tree_regression.html#visualizing-the-tree",
    "title": "Decision Tree Regression",
    "section": "Visualizing the Tree",
    "text": "Visualizing the Tree\nWe can visualize the learned splits. For readability, limit depth (or use the tuned tree if it’s shallow).\n\n# Plot the tree (may be large if depth is high)\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplot_tree(best_tree, feature_names=X.columns, filled=False, impurity=True, rounded=True)\nplt.title(\"Decision Tree (tuned)\")\nplt.show()"
  },
  {
    "objectID": "07_tree_regression.html#interpretation-feature-importance-partial-dependence",
    "href": "07_tree_regression.html#interpretation-feature-importance-partial-dependence",
    "title": "Decision Tree Regression",
    "section": "Interpretation: Feature Importance & Partial Dependence",
    "text": "Interpretation: Feature Importance & Partial Dependence\n\nFeature importance (impurity decrease) provides a global ranking of influential features.\nPartial dependence plots show the marginal effect of one or two features on the prediction while averaging over others.\n\n\n# Feature importance\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfi = pd.Series(best_tree.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint(fi.head())\n\nplt.figure(figsize=(6,4))\nfi.head(10).plot(kind='bar')\nplt.ylabel(\"Importance\")\nplt.title(\"Top-10 Feature Importances\")\nplt.show()\n\n# Partial dependence (1D) for a few top features\ntry:\n    from sklearn.inspection import PartialDependenceDisplay\n    top_feats = fi.head(3).index.tolist()\n    for feat in top_feats:\n        PartialDependenceDisplay.from_estimator(best_tree, X, [feat])\n        plt.title(f\"Partial Dependence: {feat}\")\n        plt.show()\nexcept Exception as e:\n    print(\"PartialDependenceDisplay not available:\", e)\n\nbmi    0.632805\ns5     0.214185\ns6     0.057076\ns4     0.025199\ns3     0.024707\ndtype: float64"
  },
  {
    "objectID": "07_tree_regression.html#learning-curve",
    "href": "07_tree_regression.html#learning-curve",
    "title": "Decision Tree Regression",
    "section": "Learning Curve",
    "text": "Learning Curve\nLearning curves indicate if more data would help or if the model is variance‑limited vs bias‑limited.\n\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    estimator=best_tree,\n    X=X, y=y,\n    train_sizes=np.linspace(0.1, 1.0, 8),\n    cv=5,\n    scoring='r2',\n    n_jobs=1,   # keep simple for portability\n    shuffle=True,\n    random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\ntrain_std  = train_scores.std(axis=1)\nval_mean   = val_scores.mean(axis=1)\nval_std    = val_scores.std(axis=1)\n\nplt.figure(figsize=(6,4))\nplt.plot(train_sizes, train_mean, marker='o', label='Training $R^2$')\nplt.plot(train_sizes, val_mean, marker='s', label='Validation $R^2$')\nplt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.2)\nplt.fill_between(train_sizes, val_mean-val_std,   val_mean+val_std,   alpha=0.2)\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"$R^2$\")\nplt.title(\"Learning Curve: Decision Tree\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "07_tree_regression.html#summary",
    "href": "07_tree_regression.html#summary",
    "title": "Decision Tree Regression",
    "section": "Summary",
    "text": "Summary\n\nTrees split to reduce squared error and can model non‑linear relationships.\nThey are prone to overfitting; tune max_depth, min_samples_leaf, and ccp_alpha.\nUse diagnostics, learning curves, feature importance, and partial dependence for interpretation."
  },
  {
    "objectID": "07_tree_regression.html#exercises",
    "href": "07_tree_regression.html#exercises",
    "title": "Decision Tree Regression",
    "section": "Exercises",
    "text": "Exercises\n\nValidation curve: Plot validation \\(R^2\\) as a function of max_depth and min_samples_leaf separately.\nPruning path: Use cost_complexity_pruning_path to visualize \\(\\\\alpha\\) vs effective leaves and pick a simpler tree.\nTarget transform: Fit a tree to predict \\(\\\\log(1+y)\\) and compare test \\(R^2\\).\nRobustness: Change the random seed and redo GridSearchCV. How stable are the chosen hyperparameters?"
  },
  {
    "objectID": "05_bayesian_regression.html",
    "href": "05_bayesian_regression.html",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "In this lesson we introduce Bayesian linear regression and uncertainty quantification without external PPLs (no Stan). We use: - A closed-form Bayesian linear regression under conjugate priors (Normal prior on coefficients), - A practical bootstrap approximation to predictive uncertainty.\nWe compare intervals to OLS and discuss when each approach is appropriate.\nLearning objectives - Derive and implement a closed-form Bayesian linear regression in NumPy. - Compute credible and predictive intervals via posterior sampling. - Use bootstrap resampling to approximate predictive uncertainty. - Compare OLS CIs vs Bayesian CrIs; understand their interpretations.\n\n\nAssume the standard linear model \\[\n\\mathbf{y} \\mid X, \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(X\\boldsymbol{\\beta}, \\ \\sigma^2 I_n),\n\\] with a Gaussian prior on coefficients (ridge-like prior) \\[\n\\boldsymbol{\\beta} \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\ \\tau^2 I_p\\right).\n\\]\nIf we treat \\(\\sigma^2\\) as known (or plug in an estimate), the posterior for \\(\\boldsymbol{\\beta}\\) is Gaussian: \\[\n\\Sigma_{\\text{post}} \\;=\\; \\left(\\frac{1}{\\sigma^2}X^\\top X \\;+\\; \\frac{1}{\\tau^2}I_p \\right)^{-1}, \\qquad\n\\boldsymbol{\\beta}_{\\text{post}} \\;=\\; \\Sigma_{\\text{post}} \\left(\\frac{1}{\\sigma^2}X^\\top \\mathbf{y}\\right).\n\\]\nFor a new sample \\(x_\\*\\), the posterior predictive distribution is approximately \\[\ny_\\* \\mid x_\\*, \\mathcal{D} \\ \\sim\\ \\mathcal{N}\\!\\Big(x_\\*^\\top \\boldsymbol{\\beta}_{\\text{post}},\\ \\underbrace{\\sigma^2}_{\\text{noise}} \\;+\\; \\underbrace{x_\\*^\\top \\Sigma_{\\text{post}} x_\\*}_{\\text{parameter uncertainty}}\\Big).\n\\]\n\nIn practice, we will estimate \\(\\sigma^2\\) from OLS residuals (empirical Bayes). This keeps the implementation simple and fast for teaching purposes.\n\n\n# Load Diabetes dataset and build OLS baseline\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\n\n# Data\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nX = df.drop(columns=['disease_progression']).values\ny = df['disease_progression'].values\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Add intercept column\nX_train_i = np.column_stack([np.ones(len(X_train)), X_train])\nX_test_i  = np.column_stack([np.ones(len(X_test)),  X_test])\n\nfeature_names = ['intercept'] + list(df.drop(columns=['disease_progression']).columns)\n\n# OLS baseline (for comparison)\nols = LinearRegression(fit_intercept=False)  # intercept already in X_*_i\nols.fit(X_train_i, y_train)\ny_pred_ols = ols.predict(X_test_i)\n\nols_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ols))\nols_r2   = r2_score(y_test, y_pred_ols)\n\nprint(f\"OLS Test RMSE: {ols_rmse:.3f}, R^2: {ols_r2:.3f}\")\n\nOLS Test RMSE: 53.853, R^2: 0.453\n\n\n\n# Estimate sigma^2 from OLS residuals (train set) for empirical Bayes\ny_hat_train = ols.predict(X_train_i)\nresid = y_train - y_hat_train\n\nn, p = X_train_i.shape\nsigma2_hat = (resid @ resid) / (n - p)  # unbiased estimate if model is well-specified\nprint(\"Estimated sigma^2 (train OLS residual variance):\", round(float(sigma2_hat), 3))\n\nEstimated sigma^2 (train OLS residual variance): 2960.813\n\n\n\n# Closed-form Bayesian linear regression (Normal prior on beta)\nimport numpy as np\n\n# Hyperparameter: prior std tau (tuneable). Smaller -&gt; stronger shrinkage.\ntau = 1.0\n\nI = np.eye(X_train_i.shape[1])\nXtX = X_train_i.T @ X_train_i\nXty = X_train_i.T @ y_train\n\nSigma_post = np.linalg.inv((XtX / sigma2_hat) + (I / (tau**2)))\nbeta_post  = Sigma_post @ (Xty / sigma2_hat)\n\n# Predictive mean on test set\ny_mean_post = X_test_i @ beta_post\n\n# Predictive variance: sigma^2 + x^T Sigma_post x  (diagonal, for each test point)\nparam_var = np.sum(X_test_i @ Sigma_post * X_test_i, axis=1)  # diag(X Sigma_post X^T)\ny_pred_var = sigma2_hat + param_var\n\n\n# Compute test metrics using posterior mean (point predictions)\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nrmse_bayes_mean = np.sqrt(mean_squared_error(y_test, y_mean_post))\nr2_bayes_mean   = r2_score(y_test, y_mean_post)\n\nprint(f\"Posterior-mean Test RMSE: {rmse_bayes_mean:.3f}, R^2: {r2_bayes_mean:.3f}\")\n\nPosterior-mean Test RMSE: 148.456, R^2: -3.160\n\n\n\n\n\nWe draw \\(S\\) samples \\(\\boldsymbol{\\beta}^{(s)} \\sim \\mathcal{N}(\\boldsymbol{\\beta}_{\\text{post}}, \\Sigma_{\\text{post}})\\) and compute predictive draws: \\[\ny_*^{(s)} = x_*^\\top \\boldsymbol{\\beta}^{(s)} + \\varepsilon^{(s)}, \\quad \\varepsilon^{(s)} \\sim \\mathcal{N}(0, \\sigma^2).\n\\] Taking percentiles across \\(\\{y_*^{(s)}\\}\\) yields prediction intervals; using percentiles of \\(x_*^\\top \\boldsymbol{\\beta}^{(s)}\\) yields credible intervals for the mean prediction.\n\n# Posterior sampling for intervals\nimport numpy as np\n\nS = 2000  # number of posterior samples\nbeta_samples = np.random.multivariate_normal(mean=beta_post, cov=Sigma_post, size=S)\n\n# Credible intervals for mean prediction (no noise term)\ny_mean_samples = beta_samples @ X_test_i.T  # shape (S, n_test)\n\n# Prediction intervals (include observation noise)\neps = np.random.normal(loc=0.0, scale=np.sqrt(sigma2_hat), size=y_mean_samples.shape)\ny_pred_samples = y_mean_samples + eps\n\n# Summaries (pointwise 95% intervals)\ndef pct(a, q): return np.percentile(a, q, axis=0)\nci_lo, ci_hi = pct(y_mean_samples, 2.5), pct(y_mean_samples, 97.5)\npi_lo, pi_hi = pct(y_pred_samples, 2.5), pct(y_pred_samples, 97.5)\n\n# Coverage of prediction intervals (fraction of true y in PI)\ncoverage = np.mean((y_test &gt;= pi_lo) & (y_test &lt;= pi_hi))\nprint(f\"Approx. test-set 95% PI coverage: {coverage*100:.1f}%\")\n\nApprox. test-set 95% PI coverage: 48.3%\n\n\n\n# Visualize uncertainty on a sorted index for readability\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nidx = np.argsort(y_test)\ny_sorted = y_test[idx]\ny_mean_sorted = y_mean_post[idx]\nci_lo_s, ci_hi_s = ci_lo[idx], ci_hi[idx]\npi_lo_s, pi_hi_s = pi_lo[idx], pi_hi[idx]\n\nplt.figure(figsize=(7,4))\nplt.plot(y_sorted, label=\"Actual\")\nplt.plot(y_mean_sorted, label=\"Posterior mean\")\nplt.fill_between(range(len(y_sorted)), ci_lo_s, ci_hi_s, alpha=0.3, label=\"95% CrI (mean)\")\nplt.fill_between(range(len(y_sorted)), pi_lo_s, pi_hi_s, alpha=0.2, label=\"95% PI (predictive)\")\nplt.xlabel(\"Sorted test index\")\nplt.ylabel(\"Target\")\nplt.title(\"Bayesian Regression: Credible vs Prediction Intervals\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe bootstrap resamples the training data to mimic sampling variability: 1. Draw \\(B\\) bootstrap datasets by sampling \\(n\\) rows with replacement. 2. Fit an OLS model on each bootstrap sample. 3. Predict on the test set; take percentiles across predictions for empirical prediction intervals.\nThis is not Bayesian, but it often gives similar intuition about uncertainty without extra libraries.\n\n# Bootstrap predictive intervals\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nB = 200  # number of bootstrap replicates (increase if time allows)\npreds = []\n\nfor b in range(B):\n    Xb, yb = resample(X_train_i, y_train, replace=True, random_state=1234 + b)\n    m = LinearRegression(fit_intercept=False)\n    m.fit(Xb, yb)\n    preds.append(m.predict(X_test_i))\n\npreds = np.array(preds)  # shape (B, n_test)\n\ndef pct(a, q): return np.percentile(a, q, axis=0)\nboot_lo, boot_hi = pct(preds, 2.5), pct(preds, 97.5)\n\n# Coverage (how often true y lies within bootstrap interval)\nboot_cov = np.mean((y_test &gt;= boot_lo) & (y_test &lt;= boot_hi))\nprint(f\"Bootstrap approx. 95% interval coverage: {boot_cov*100:.1f}%\")\n\nBootstrap approx. 95% interval coverage: 28.1%\n\n\n\n# Compare Bayesian PI vs Bootstrap intervals on a few points\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nk = min(30, len(y_test))  # plot first k sorted points for clarity\nidx = np.argsort(y_test)[:k]\n\ny_c = y_mean_post[idx]\n\n# Bayes PI errors (ensure non-negative magnitudes)\nbayes_lower = np.clip(y_c - pi_lo[idx], 0, None)\nbayes_upper = np.clip(pi_hi[idx] - y_c, 0, None)\nbayes_yerr = np.vstack([bayes_lower, bayes_upper])\n\n# Bootstrap PI errors (ensure non-negative magnitudes)\nboot_lower = np.clip(y_c - boot_lo[idx], 0, None)\nboot_upper = np.clip(boot_hi[idx] - y_c, 0, None)\nboot_yerr = np.vstack([boot_lower, boot_upper])\n\nplt.figure(figsize=(7,4))\nplt.plot(range(k), y_test[idx], label=\"Actual\", marker='o')\nplt.plot(range(k), y_c, label=\"Posterior mean\", linestyle='--')\n\nplt.errorbar(range(k), y_c, yerr=bayes_yerr, fmt='none', capsize=3, label=\"Bayes 95% PI\")\nplt.errorbar(range(k), y_c, yerr=boot_yerr,  fmt='none', capsize=3, label=\"Bootstrap 95% PI\")\n\nplt.xlabel(\"Sorted test index (subset)\")\nplt.ylabel(\"Target\")\nplt.title(\"Bayesian vs Bootstrap Prediction Intervals (subset)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence interval (CI): frequency-based coverage under repeated sampling. It does not say “the parameter has 95% probability” in the interval.\nCredible interval (CrI): posterior probability statement given data and priors. With a Gaussian prior and large \\(n\\), CrIs often resemble CIs.\n\nPrediction intervals include observation noise, hence are wider than intervals for the mean prediction.\n\n\n\n\nPrior sensitivity: Change \\(\\tau\\) (e.g., 0.3, 1.0, 3.0). How do the credible and predictive intervals change? Report test \\(R^2\\).\nEmpirical Bayes for \\(\\tau\\): Choose \\(\\tau\\) by 5‑fold CV, maximizing validation \\(R^2\\) (hint: mimic Ridge search).\nHeteroscedasticity: Replace constant \\(\\sigma^2\\) with a simple variance model \\(\\sigma^2(x) = \\sigma^2 (1 + \\gamma |x^\\top \\beta|)\\) and discuss the effect on intervals (simulation).\nBootstrap size: Increase \\(B\\) to 1000 (if time allows). Do coverage and interval width stabilize?"
  },
  {
    "objectID": "05_bayesian_regression.html#bayesian-linear-regression-conjugate-stan-free",
    "href": "05_bayesian_regression.html#bayesian-linear-regression-conjugate-stan-free",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "Assume the standard linear model \\[\n\\mathbf{y} \\mid X, \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(X\\boldsymbol{\\beta}, \\ \\sigma^2 I_n),\n\\] with a Gaussian prior on coefficients (ridge-like prior) \\[\n\\boldsymbol{\\beta} \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\ \\tau^2 I_p\\right).\n\\]\nIf we treat \\(\\sigma^2\\) as known (or plug in an estimate), the posterior for \\(\\boldsymbol{\\beta}\\) is Gaussian: \\[\n\\Sigma_{\\text{post}} \\;=\\; \\left(\\frac{1}{\\sigma^2}X^\\top X \\;+\\; \\frac{1}{\\tau^2}I_p \\right)^{-1}, \\qquad\n\\boldsymbol{\\beta}_{\\text{post}} \\;=\\; \\Sigma_{\\text{post}} \\left(\\frac{1}{\\sigma^2}X^\\top \\mathbf{y}\\right).\n\\]\nFor a new sample \\(x_\\*\\), the posterior predictive distribution is approximately \\[\ny_\\* \\mid x_\\*, \\mathcal{D} \\ \\sim\\ \\mathcal{N}\\!\\Big(x_\\*^\\top \\boldsymbol{\\beta}_{\\text{post}},\\ \\underbrace{\\sigma^2}_{\\text{noise}} \\;+\\; \\underbrace{x_\\*^\\top \\Sigma_{\\text{post}} x_\\*}_{\\text{parameter uncertainty}}\\Big).\n\\]\n\nIn practice, we will estimate \\(\\sigma^2\\) from OLS residuals (empirical Bayes). This keeps the implementation simple and fast for teaching purposes.\n\n\n# Load Diabetes dataset and build OLS baseline\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\n\n# Data\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nX = df.drop(columns=['disease_progression']).values\ny = df['disease_progression'].values\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Add intercept column\nX_train_i = np.column_stack([np.ones(len(X_train)), X_train])\nX_test_i  = np.column_stack([np.ones(len(X_test)),  X_test])\n\nfeature_names = ['intercept'] + list(df.drop(columns=['disease_progression']).columns)\n\n# OLS baseline (for comparison)\nols = LinearRegression(fit_intercept=False)  # intercept already in X_*_i\nols.fit(X_train_i, y_train)\ny_pred_ols = ols.predict(X_test_i)\n\nols_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ols))\nols_r2   = r2_score(y_test, y_pred_ols)\n\nprint(f\"OLS Test RMSE: {ols_rmse:.3f}, R^2: {ols_r2:.3f}\")\n\nOLS Test RMSE: 53.853, R^2: 0.453\n\n\n\n# Estimate sigma^2 from OLS residuals (train set) for empirical Bayes\ny_hat_train = ols.predict(X_train_i)\nresid = y_train - y_hat_train\n\nn, p = X_train_i.shape\nsigma2_hat = (resid @ resid) / (n - p)  # unbiased estimate if model is well-specified\nprint(\"Estimated sigma^2 (train OLS residual variance):\", round(float(sigma2_hat), 3))\n\nEstimated sigma^2 (train OLS residual variance): 2960.813\n\n\n\n# Closed-form Bayesian linear regression (Normal prior on beta)\nimport numpy as np\n\n# Hyperparameter: prior std tau (tuneable). Smaller -&gt; stronger shrinkage.\ntau = 1.0\n\nI = np.eye(X_train_i.shape[1])\nXtX = X_train_i.T @ X_train_i\nXty = X_train_i.T @ y_train\n\nSigma_post = np.linalg.inv((XtX / sigma2_hat) + (I / (tau**2)))\nbeta_post  = Sigma_post @ (Xty / sigma2_hat)\n\n# Predictive mean on test set\ny_mean_post = X_test_i @ beta_post\n\n# Predictive variance: sigma^2 + x^T Sigma_post x  (diagonal, for each test point)\nparam_var = np.sum(X_test_i @ Sigma_post * X_test_i, axis=1)  # diag(X Sigma_post X^T)\ny_pred_var = sigma2_hat + param_var\n\n\n# Compute test metrics using posterior mean (point predictions)\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nrmse_bayes_mean = np.sqrt(mean_squared_error(y_test, y_mean_post))\nr2_bayes_mean   = r2_score(y_test, y_mean_post)\n\nprint(f\"Posterior-mean Test RMSE: {rmse_bayes_mean:.3f}, R^2: {r2_bayes_mean:.3f}\")\n\nPosterior-mean Test RMSE: 148.456, R^2: -3.160"
  },
  {
    "objectID": "05_bayesian_regression.html#credible-predictive-intervals-via-sampling",
    "href": "05_bayesian_regression.html#credible-predictive-intervals-via-sampling",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "We draw \\(S\\) samples \\(\\boldsymbol{\\beta}^{(s)} \\sim \\mathcal{N}(\\boldsymbol{\\beta}_{\\text{post}}, \\Sigma_{\\text{post}})\\) and compute predictive draws: \\[\ny_*^{(s)} = x_*^\\top \\boldsymbol{\\beta}^{(s)} + \\varepsilon^{(s)}, \\quad \\varepsilon^{(s)} \\sim \\mathcal{N}(0, \\sigma^2).\n\\] Taking percentiles across \\(\\{y_*^{(s)}\\}\\) yields prediction intervals; using percentiles of \\(x_*^\\top \\boldsymbol{\\beta}^{(s)}\\) yields credible intervals for the mean prediction.\n\n# Posterior sampling for intervals\nimport numpy as np\n\nS = 2000  # number of posterior samples\nbeta_samples = np.random.multivariate_normal(mean=beta_post, cov=Sigma_post, size=S)\n\n# Credible intervals for mean prediction (no noise term)\ny_mean_samples = beta_samples @ X_test_i.T  # shape (S, n_test)\n\n# Prediction intervals (include observation noise)\neps = np.random.normal(loc=0.0, scale=np.sqrt(sigma2_hat), size=y_mean_samples.shape)\ny_pred_samples = y_mean_samples + eps\n\n# Summaries (pointwise 95% intervals)\ndef pct(a, q): return np.percentile(a, q, axis=0)\nci_lo, ci_hi = pct(y_mean_samples, 2.5), pct(y_mean_samples, 97.5)\npi_lo, pi_hi = pct(y_pred_samples, 2.5), pct(y_pred_samples, 97.5)\n\n# Coverage of prediction intervals (fraction of true y in PI)\ncoverage = np.mean((y_test &gt;= pi_lo) & (y_test &lt;= pi_hi))\nprint(f\"Approx. test-set 95% PI coverage: {coverage*100:.1f}%\")\n\nApprox. test-set 95% PI coverage: 48.3%\n\n\n\n# Visualize uncertainty on a sorted index for readability\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nidx = np.argsort(y_test)\ny_sorted = y_test[idx]\ny_mean_sorted = y_mean_post[idx]\nci_lo_s, ci_hi_s = ci_lo[idx], ci_hi[idx]\npi_lo_s, pi_hi_s = pi_lo[idx], pi_hi[idx]\n\nplt.figure(figsize=(7,4))\nplt.plot(y_sorted, label=\"Actual\")\nplt.plot(y_mean_sorted, label=\"Posterior mean\")\nplt.fill_between(range(len(y_sorted)), ci_lo_s, ci_hi_s, alpha=0.3, label=\"95% CrI (mean)\")\nplt.fill_between(range(len(y_sorted)), pi_lo_s, pi_hi_s, alpha=0.2, label=\"95% PI (predictive)\")\nplt.xlabel(\"Sorted test index\")\nplt.ylabel(\"Target\")\nplt.title(\"Bayesian Regression: Credible vs Prediction Intervals\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "05_bayesian_regression.html#bootstrap-approximation-frequentist-but-practical",
    "href": "05_bayesian_regression.html#bootstrap-approximation-frequentist-but-practical",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "The bootstrap resamples the training data to mimic sampling variability: 1. Draw \\(B\\) bootstrap datasets by sampling \\(n\\) rows with replacement. 2. Fit an OLS model on each bootstrap sample. 3. Predict on the test set; take percentiles across predictions for empirical prediction intervals.\nThis is not Bayesian, but it often gives similar intuition about uncertainty without extra libraries.\n\n# Bootstrap predictive intervals\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nB = 200  # number of bootstrap replicates (increase if time allows)\npreds = []\n\nfor b in range(B):\n    Xb, yb = resample(X_train_i, y_train, replace=True, random_state=1234 + b)\n    m = LinearRegression(fit_intercept=False)\n    m.fit(Xb, yb)\n    preds.append(m.predict(X_test_i))\n\npreds = np.array(preds)  # shape (B, n_test)\n\ndef pct(a, q): return np.percentile(a, q, axis=0)\nboot_lo, boot_hi = pct(preds, 2.5), pct(preds, 97.5)\n\n# Coverage (how often true y lies within bootstrap interval)\nboot_cov = np.mean((y_test &gt;= boot_lo) & (y_test &lt;= boot_hi))\nprint(f\"Bootstrap approx. 95% interval coverage: {boot_cov*100:.1f}%\")\n\nBootstrap approx. 95% interval coverage: 28.1%\n\n\n\n# Compare Bayesian PI vs Bootstrap intervals on a few points\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nk = min(30, len(y_test))  # plot first k sorted points for clarity\nidx = np.argsort(y_test)[:k]\n\ny_c = y_mean_post[idx]\n\n# Bayes PI errors (ensure non-negative magnitudes)\nbayes_lower = np.clip(y_c - pi_lo[idx], 0, None)\nbayes_upper = np.clip(pi_hi[idx] - y_c, 0, None)\nbayes_yerr = np.vstack([bayes_lower, bayes_upper])\n\n# Bootstrap PI errors (ensure non-negative magnitudes)\nboot_lower = np.clip(y_c - boot_lo[idx], 0, None)\nboot_upper = np.clip(boot_hi[idx] - y_c, 0, None)\nboot_yerr = np.vstack([boot_lower, boot_upper])\n\nplt.figure(figsize=(7,4))\nplt.plot(range(k), y_test[idx], label=\"Actual\", marker='o')\nplt.plot(range(k), y_c, label=\"Posterior mean\", linestyle='--')\n\nplt.errorbar(range(k), y_c, yerr=bayes_yerr, fmt='none', capsize=3, label=\"Bayes 95% PI\")\nplt.errorbar(range(k), y_c, yerr=boot_yerr,  fmt='none', capsize=3, label=\"Bootstrap 95% PI\")\n\nplt.xlabel(\"Sorted test index (subset)\")\nplt.ylabel(\"Target\")\nplt.title(\"Bayesian vs Bootstrap Prediction Intervals (subset)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "05_bayesian_regression.html#confidence-vs-credible-intervals-recap",
    "href": "05_bayesian_regression.html#confidence-vs-credible-intervals-recap",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "Confidence interval (CI): frequency-based coverage under repeated sampling. It does not say “the parameter has 95% probability” in the interval.\nCredible interval (CrI): posterior probability statement given data and priors. With a Gaussian prior and large \\(n\\), CrIs often resemble CIs.\n\nPrediction intervals include observation noise, hence are wider than intervals for the mean prediction."
  },
  {
    "objectID": "05_bayesian_regression.html#exercises",
    "href": "05_bayesian_regression.html#exercises",
    "title": "Bayesian Regression (Stan-free) & Uncertainty",
    "section": "",
    "text": "Prior sensitivity: Change \\(\\tau\\) (e.g., 0.3, 1.0, 3.0). How do the credible and predictive intervals change? Report test \\(R^2\\).\nEmpirical Bayes for \\(\\tau\\): Choose \\(\\tau\\) by 5‑fold CV, maximizing validation \\(R^2\\) (hint: mimic Ridge search).\nHeteroscedasticity: Replace constant \\(\\sigma^2\\) with a simple variance model \\(\\sigma^2(x) = \\sigma^2 (1 + \\gamma |x^\\top \\beta|)\\) and discuss the effect on intervals (simulation).\nBootstrap size: Increase \\(B\\) to 1000 (if time allows). Do coverage and interval width stabilize?"
  },
  {
    "objectID": "03_model_evaluation.html",
    "href": "03_model_evaluation.html",
    "title": "Model Evaluation for Regression",
    "section": "",
    "text": "In this lesson we focus on how to evaluate regression models. We’ll compute standard quantitative metrics and use visual diagnostics to understand error structure. We’ll also introduce cross‑validation and learning curves to reason about generalisation and data sufficiency.\nLearning objectives - Compute and interpret \\(\\text{MSE}\\), \\(\\text{RMSE}\\), \\(\\text{MAE}\\), and \\(R^2\\) on train/test splits. - Create residual diagnostics: residuals vs predictions, predicted vs actual, and residual histograms (plus an optional Q–Q plot). - Use cross‑validation to obtain more stable performance estimates. - Plot and read learning curves; understand under/overfitting and data needs."
  },
  {
    "objectID": "03_model_evaluation.html#quantitative-metrics-theory",
    "href": "03_model_evaluation.html#quantitative-metrics-theory",
    "title": "Model Evaluation for Regression",
    "section": "Quantitative Metrics (Theory)",
    "text": "Quantitative Metrics (Theory)\nLet \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be the dataset and \\(\\hat{y}_i\\) the model predictions.\n\nMean Squared Error (MSE) \\[\n\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nRoot Mean Squared Error (RMSE) — same units as \\(y\\): \\[\n\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}\n\\]\nMean Absolute Error (MAE) — robust to outliers: \\[\n\\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^n \\lvert y_i - \\hat{y}_i \\rvert\n\\]\nCoefficient of Determination (\\(R^2\\)) — variance explained: \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]\n\nRules of thumb - Lower RMSE/MAE is better; compare across models trained on the same target and scale. - Higher \\(R^2\\) is better; negative \\(R^2\\) can occur on test data (worse than predicting the mean).\n\n# Load Diabetes dataset, split, fit a baseline Linear Regression model\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport pandas as pd\nimport numpy as np\n\n# Data\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nX = df.drop(columns=['disease_progression'])\ny = df['disease_progression']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Baseline model\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n# Predictions\ny_pred_train = linreg.predict(X_train)\ny_pred_test  = linreg.predict(X_test)\n\n# Metrics table\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test  = mean_squared_error(y_test, y_pred_test)\nrmse_train = np.sqrt(mse_train)\nrmse_test  = np.sqrt(mse_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test  = mean_absolute_error(y_test, y_pred_test)\nr2_train = r2_score(y_train, y_pred_train)\nr2_test  = r2_score(y_test, y_pred_test)\n\nmetrics_df = pd.DataFrame({\n    'set': ['train', 'test'],\n    'MSE': [mse_train, mse_test],\n    'RMSE': [rmse_train, rmse_test],\n    'MAE': [mae_train, mae_test],\n    'R2': [r2_train, r2_test],\n})\nmetrics_df\n\n\n\n\n\n\n\n\nset\nMSE\nRMSE\nMAE\nR2\n\n\n\n\n0\ntrain\n2868.549703\n53.558843\n43.483504\n0.527919\n\n\n1\ntest\n2900.193628\n53.853446\n42.794095\n0.452603\n\n\n\n\n\n\n\n\nReading the table\n\nCompare train vs test. A large gap (train much better) suggests overfitting; both poor suggests underfitting or model misspecification.\nPrefer RMSE or MAE for interpretability (units of the target)."
  },
  {
    "objectID": "03_model_evaluation.html#visual-diagnostics",
    "href": "03_model_evaluation.html#visual-diagnostics",
    "title": "Model Evaluation for Regression",
    "section": "Visual Diagnostics",
    "text": "Visual Diagnostics\nResidual plots help us see patterns that metrics cannot reveal. - Residuals vs Predicted: check linearity and homoscedasticity. - Predicted vs Actual: overall calibration (points near the diagonal are better). - Residual Histogram: rough normality check (useful for inference). - Q–Q plot (optional): a more formal normality visual.\n\n# Residual diagnostics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nresiduals_train = y_train - y_pred_train\nresiduals_test  = y_test - y_pred_test\n\n# Residuals vs Predicted (Test)\nplt.figure(figsize=(5,4))\nplt.scatter(y_pred_test, residuals_test, alpha=0.7)\nplt.axhline(0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals (y - y_hat)\")\nplt.title(\"Residuals vs Predicted (Test)\")\nplt.show()\n\n# Predicted vs Actual (Test)\nplt.figure(figsize=(5,5))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nlo = min(y_test.min(), y_pred_test.min())\nhi = max(y_test.max(), y_pred_test.max())\nplt.plot([lo, hi], [lo, hi])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Predicted vs Actual (Test)\")\nplt.show()\n\n# Residual histogram (Test)\nplt.figure(figsize=(5,4))\nplt.hist(residuals_test, bins=20)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Residuals Distribution (Test)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: Q–Q Plot\nA Q–Q plot compares residual quantiles to those of a normal distribution. - Straight line \\(\\Rightarrow\\) residuals approximately normal. - Heavy tails or curvature \\(\\Rightarrow\\) departures from normality (affects inference more than prediction).\n\n# Optional Q–Q plot (requires scipy). If not installed on the platform, skip this cell.\ntry:\n    import scipy.stats as stats\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(5,4))\n    ax = fig.add_subplot(111)\n    stats.probplot((y_test - y_pred_test), dist=\"norm\", plot=ax)\n    ax.set_title(\"Q–Q Plot of Residuals (Test)\")\n    plt.show()\nexcept Exception as e:\n    print(\"Skipping Q–Q plot (scipy likely not available in this environment):\", e)\n\n\n\n\n\n\n\n\nInterpreting diagnostics - Non-linearity: Curvature in residuals vs predicted suggests the linear model is missing structure (consider trees, interactions, or basis expansions). - Heteroscedasticity: Cone/funnel shapes indicate variance depends on \\(\\hat{y}\\); robust losses or transforms may help. - Outliers/Influential points: Large residuals can unduly affect fit; consider robust models or inspection."
  },
  {
    "objectID": "03_model_evaluation.html#crossvalidation",
    "href": "03_model_evaluation.html#crossvalidation",
    "title": "Model Evaluation for Regression",
    "section": "Cross‑Validation",
    "text": "Cross‑Validation\nTrain/test splits can be noisy. \\(k\\)‑fold cross‑validation averages performance across \\(k\\) folds to reduce variance.\nWe’ll compute cross‑validated \\(R^2\\) and negative MSE (higher is better for \\(R^2\\); less negative is better for MSE). Use n_jobs=-1 to enable implicit parallelism on NCI ARE.\n\nfrom sklearn.model_selection import cross_val_score, KFold\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_r2  = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=1)\ncv_mse = cross_val_score(LinearRegression(), X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=1)\n\nprint(\"CV R^2 scores:\", cv_r2)\nprint(\"Mean R^2:\", np.mean(cv_r2), \"±\", np.std(cv_r2))\nprint()\nprint(\"CV neg-MSE scores:\", cv_mse)\nprint(\"Mean neg-MSE:\", np.mean(cv_mse), \"±\", np.std(cv_mse))\nprint(\"Mean RMSE (from neg-MSE):\", np.sqrt(-np.mean(cv_mse)))\n\nCV R^2 scores: [0.45260276 0.57320015 0.39144785 0.58428888 0.39081186]\nMean R^2: 0.47847030225778486 ± 0.08496746925761751\n\nCV neg-MSE scores: [-2900.19362849 -2662.63760862 -3312.30588884 -2797.88355256\n -3403.88779293]\nMean neg-MSE: -3015.381694287271 ± 291.23979402678145\nMean RMSE (from neg-MSE): 54.91249124094873\n\n\n\nMean & spread across folds indicate stability.\nIf performance varies widely across folds, model/data are sensitive to splits (consider more data, regularisation, or different features)."
  },
  {
    "objectID": "03_model_evaluation.html#learning-curves",
    "href": "03_model_evaluation.html#learning-curves",
    "title": "Model Evaluation for Regression",
    "section": "Learning Curves",
    "text": "Learning Curves\nLearning curves show training and validation performance as a function of training set size. They help diagnose: - High bias (underfitting): both curves converge to a poor score. - High variance (overfitting): large gap between curves that narrows with more data.\n\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    estimator=LinearRegression(),\n    X=X, y=y,\n    train_sizes=np.linspace(0.1, 1.0, 8),\n    cv=5,\n    scoring='r2',\n    n_jobs=1,\n    shuffle=True,\n    random_state=42\n)\n\ntrain_mean = train_scores.mean(axis=1)\ntrain_std  = train_scores.std(axis=1)\nval_mean   = val_scores.mean(axis=1)\nval_std    = val_scores.std(axis=1)\n\nplt.figure(figsize=(6,4))\nplt.plot(train_sizes, train_mean, marker='o', label='Training $R^2$')\nplt.plot(train_sizes, val_mean, marker='s', label='Validation $R^2$')\nplt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.2)\nplt.fill_between(train_sizes, val_mean-val_std,   val_mean+val_std,   alpha=0.2)\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"$R^2$\")\nplt.title(\"Learning Curve: Linear Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nReading the curve - If both curves plateau at a low \\(R^2\\), the model is bias‑limited (try richer models or features). - If the gap remains large, the model is variance‑limited (try regularisation, more data, or ensembles)."
  },
  {
    "objectID": "03_model_evaluation.html#summary",
    "href": "03_model_evaluation.html#summary",
    "title": "Model Evaluation for Regression",
    "section": "Summary",
    "text": "Summary\n\nMetrics (\\(\\text{RMSE}\\), \\(\\text{MAE}\\), \\(R^2\\)) quantify performance, but diagnostic plots reveal error structure.\nCross‑validation reduces variance in performance estimates.\nLearning curves guide decisions about model complexity and data requirements."
  },
  {
    "objectID": "03_model_evaluation.html#exercises",
    "href": "03_model_evaluation.html#exercises",
    "title": "Model Evaluation for Regression",
    "section": "Exercises",
    "text": "Exercises\n\nAlternative metric: Add \\(R^2\\) and RMSE for training and test to a single bar chart for quick comparison.\nTransform experiment: Apply a log transform to the target (e.g., np.log1p) and re‑evaluate. Did residuals improve?\nRobustness check: Use 10‑fold CV and compare the mean/variance of \\(R^2\\) to 5‑fold. Which is more stable here?"
  },
  {
    "objectID": "01_introduction_to_regression.html",
    "href": "01_introduction_to_regression.html",
    "title": "Introduction to Regression",
    "section": "",
    "text": "Welcome! In this lesson we will introduce regression as a supervised learning task for predicting continuous outcomes. We’ll set the stage for the course, outline the models we will study, and load our first dataset (the Diabetes dataset from scikit-learn).\nLearning objectives - Understand the goal of regression and how it differs from classification. - Recognize common regression model families and when to use them. - Get familiar with the Diabetes dataset we will use throughout the first part of the course. - Perform a light EDA (exploratory data analysis) to understand features and the target.\n\n\n\nWhen: Thursday 11 September 2025, 9:30am–1:30pm AEST (online)\nWhere: NCI ARE platform via your web browser (no local installs). A reliable internet connection is required; a second monitor is recommended.\nBring: Your own computer, browser, and your NCI account ready. Setup instructions will be provided ahead of time.\nWho: Researchers from Australian universities and Australian researchers using (or planning to use) NCI.\n\nPrerequisites - Basic Python (e.g., Intro to Python / Software Carpentry) and fundamental statistics. - No prior machine learning experience is required.\nPerformance & parallelisation (preview) - Many scikit-learn estimators support implicit parallelism with n_jobs=-1 (use all available cores on ARE). We will use this later for tree ensembles and cross‑validation. - For Bayesian inference with cmdstanpy, we will demonstrate running multiple chains in parallel on ARE. - We will also discuss lightweight tips for using NCI resources efficiently (e.g., monitoring, avoiding oversubscription).\n\nDetails and hands‑on examples will appear in later notebooks (Random Forests, Model Evaluation, and Bayesian Regression).\n\n\n\n\n\nClassification: predict a category/label (e.g., healthy vs. sick).\nRegression: predict a continuous value (e.g., blood pressure, medical costs).\n\nWe assume a data-generating process of the form \\[\n\\hat{y} = f(X) + \\varepsilon\n\\] where: - \\(\\hat{y}\\) is the model’s prediction for the target \\(y\\), - \\(X \\in \\mathbb{R}^{n \\times p}\\) is the feature matrix with \\(n\\) samples and \\(p\\) features, - \\(f(\\cdot)\\) is the (unknown) function we wish to approximate, - \\(\\varepsilon\\) is random noise (mean zero, finite variance).\n\n\n\nLinear models (e.g., Ordinary Least Squares)\nRegularized linear models (Ridge, Lasso)\nTree-based models (Decision Tree, Random Forest)\nNeural networks (Multi-Layer Perceptron for regression)\n\nEach family captures different types of relationships and trade-offs between bias, variance, and interpretability.\n\n\n\n\nWhen choosing models, we balance bias (error from overly simple assumptions) and variance (error from sensitivity to the specific training data).\nThe expected generalization error can be decomposed (for squared loss) into bias, variance, and irreducible noise: \\[\n\\mathbb{E}\\left[(y - \\hat{f}(X))^2\\right] = \\underbrace{\\text{Bias}^2}_{\\text{underfitting}} + \\underbrace{\\text{Variance}}_{\\\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{noise}}.\n\\]\n\nHigh bias \\(\\rightarrow\\) underfitting (model too simple).\nHigh variance \\(\\rightarrow\\) overfitting (model too complex).\nWe will use evaluation metrics and diagnostic plots (e.g., residual plots, learning curves) to reason about this balance.\n\n\n\n\nWe will start with the Diabetes dataset that ships with scikit-learn (small, clean, numeric). Later, we will bring in the Medical Insurance dataset (real-world, mixed numeric & categorical features) to practice preprocessing and feature engineering.\nDiabetes dataset (regression) - Target: Quantitative measure of disease progression one year after baseline. - Features: 10 baseline variables such as age, sex, BMI, blood pressure, and serum measurements. - Why start here? Small, quick to train, great for teaching core concepts.\nMedical Insurance dataset (regression) (used later) - Target: charges (medical insurance costs) - Features: age, sex, bmi, children, smoker, region (requires encoding) - Why later? Demonstrates categorical encoding, interactions, and non-linear effects.\n\n# Load and preview the Diabetes dataset (built into scikit-learn)\nfrom sklearn.datasets import load_diabetes\nimport pandas as pd\n\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()  # includes features and target under 'target'\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nprint(\"Shape:\", df.shape)\ndf.head()\n\nShape: (442, 11)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\n\n\nage: age in years (standardized)\nsex: sex (standardized)\nbmi: body mass index (standardized)\nbp: average blood pressure (standardized)\ns1–s6: six serum measurements (standardized)\ndisease_progression (target): a quantitative measure of disease progression one year after baseline\n\n\nNote: Features are already standardized (mean ~0, variance ~1). This helps many models converge quickly and makes coefficients comparable in linear models.\n\n\n# Basic EDA\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\n\n\n\nScales: Here they are standardized, which simplifies model training.\nDistributions: Skewness/heavy tails can affect error metrics (e.g., MAE vs RMSE).\nTarget behavior: Understanding the spread of disease_progression will help interpret errors later.\n\n\n# Histograms for features and target\nimport matplotlib.pyplot as plt\n\n_ = df.hist(figsize=(12, 8), bins=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhy histograms? - Quick sense of distribution shape (normality, skewness, outliers). - Helps anticipate when certain models/metrics might struggle.\n\n# Scatter plots: individual features vs target\nimport matplotlib.pyplot as plt\n\nfeatures_to_plot = ['bmi', 'bp', 's5']\nfor col in features_to_plot:\n    plt.figure(figsize=(5,4))\n    plt.scatter(df[col], df['disease_progression'], alpha=0.7)\n    plt.xlabel(col)\n    plt.ylabel('disease_progression')\n    plt.title(f'{col} vs disease_progression')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading these plots - A monotonic trend (up or down) suggests a linear component. - Curvature suggests possible non-linear relationships (trees, interactions, neural nets may help). - Heteroscedasticity (variance changing with \\(x\\)) foreshadows issues for ordinary least squares assumptions.\n\n\n\n\nIn the next notebook, we will fit a Linear Regression model on this dataset and discuss: - The linear model \\(\\hat{y} = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\\) - Ordinary Least Squares (OLS) objective and closed-form solution - Interpreting coefficients and assumptions - First set of evaluation metrics and diagnostic plots (moved fully to the following notebook)\nWe will then follow with a dedicated Model Evaluation notebook to build robust instincts for diagnosing bias/variance, under/overfitting, and when to consider regularization or non-linear models.\n\n\n\n\nUsing the scatter plots above, which feature seems most correlated with disease_progression? Why?\nCompute the correlation matrix and identify the top-3 features most correlated with the target.\n(Think) What kinds of clinical or biological processes might explain non-linear patterns in this dataset? How would that influence your model choice?\n\n\n# (Optional) Correlation matrix to support Exercise 2\ncorr = df.corr(numeric_only=True)\ncorr['disease_progression'].sort_values(ascending=False)\n\ndisease_progression    1.000000\nbmi                    0.586450\ns5                     0.565883\nbp                     0.441482\ns4                     0.430453\ns6                     0.382483\ns1                     0.212022\nage                    0.187889\ns2                     0.174054\nsex                    0.043062\ns3                    -0.394789\nName: disease_progression, dtype: float64"
  },
  {
    "objectID": "01_introduction_to_regression.html#workshop-logistics-nci-are-setup",
    "href": "01_introduction_to_regression.html#workshop-logistics-nci-are-setup",
    "title": "Introduction to Regression",
    "section": "",
    "text": "When: Thursday 11 September 2025, 9:30am–1:30pm AEST (online)\nWhere: NCI ARE platform via your web browser (no local installs). A reliable internet connection is required; a second monitor is recommended.\nBring: Your own computer, browser, and your NCI account ready. Setup instructions will be provided ahead of time.\nWho: Researchers from Australian universities and Australian researchers using (or planning to use) NCI.\n\nPrerequisites - Basic Python (e.g., Intro to Python / Software Carpentry) and fundamental statistics. - No prior machine learning experience is required.\nPerformance & parallelisation (preview) - Many scikit-learn estimators support implicit parallelism with n_jobs=-1 (use all available cores on ARE). We will use this later for tree ensembles and cross‑validation. - For Bayesian inference with cmdstanpy, we will demonstrate running multiple chains in parallel on ARE. - We will also discuss lightweight tips for using NCI resources efficiently (e.g., monitoring, avoiding oversubscription).\n\nDetails and hands‑on examples will appear in later notebooks (Random Forests, Model Evaluation, and Bayesian Regression)."
  },
  {
    "objectID": "01_introduction_to_regression.html#what-is-regression-how-is-it-different-from-classification",
    "href": "01_introduction_to_regression.html#what-is-regression-how-is-it-different-from-classification",
    "title": "Introduction to Regression",
    "section": "",
    "text": "Classification: predict a category/label (e.g., healthy vs. sick).\nRegression: predict a continuous value (e.g., blood pressure, medical costs).\n\nWe assume a data-generating process of the form \\[\n\\hat{y} = f(X) + \\varepsilon\n\\] where: - \\(\\hat{y}\\) is the model’s prediction for the target \\(y\\), - \\(X \\in \\mathbb{R}^{n \\times p}\\) is the feature matrix with \\(n\\) samples and \\(p\\) features, - \\(f(\\cdot)\\) is the (unknown) function we wish to approximate, - \\(\\varepsilon\\) is random noise (mean zero, finite variance).\n\n\n\nLinear models (e.g., Ordinary Least Squares)\nRegularized linear models (Ridge, Lasso)\nTree-based models (Decision Tree, Random Forest)\nNeural networks (Multi-Layer Perceptron for regression)\n\nEach family captures different types of relationships and trade-offs between bias, variance, and interpretability."
  },
  {
    "objectID": "01_introduction_to_regression.html#biasvariance-intuition",
    "href": "01_introduction_to_regression.html#biasvariance-intuition",
    "title": "Introduction to Regression",
    "section": "",
    "text": "When choosing models, we balance bias (error from overly simple assumptions) and variance (error from sensitivity to the specific training data).\nThe expected generalization error can be decomposed (for squared loss) into bias, variance, and irreducible noise: \\[\n\\mathbb{E}\\left[(y - \\hat{f}(X))^2\\right] = \\underbrace{\\text{Bias}^2}_{\\text{underfitting}} + \\underbrace{\\text{Variance}}_{\\\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{noise}}.\n\\]\n\nHigh bias \\(\\rightarrow\\) underfitting (model too simple).\nHigh variance \\(\\rightarrow\\) overfitting (model too complex).\nWe will use evaluation metrics and diagnostic plots (e.g., residual plots, learning curves) to reason about this balance."
  },
  {
    "objectID": "01_introduction_to_regression.html#datasets-used-in-this-course",
    "href": "01_introduction_to_regression.html#datasets-used-in-this-course",
    "title": "Introduction to Regression",
    "section": "",
    "text": "We will start with the Diabetes dataset that ships with scikit-learn (small, clean, numeric). Later, we will bring in the Medical Insurance dataset (real-world, mixed numeric & categorical features) to practice preprocessing and feature engineering.\nDiabetes dataset (regression) - Target: Quantitative measure of disease progression one year after baseline. - Features: 10 baseline variables such as age, sex, BMI, blood pressure, and serum measurements. - Why start here? Small, quick to train, great for teaching core concepts.\nMedical Insurance dataset (regression) (used later) - Target: charges (medical insurance costs) - Features: age, sex, bmi, children, smoker, region (requires encoding) - Why later? Demonstrates categorical encoding, interactions, and non-linear effects.\n\n# Load and preview the Diabetes dataset (built into scikit-learn)\nfrom sklearn.datasets import load_diabetes\nimport pandas as pd\n\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()  # includes features and target under 'target'\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nprint(\"Shape:\", df.shape)\ndf.head()\n\nShape: (442, 11)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\n\n\nage: age in years (standardized)\nsex: sex (standardized)\nbmi: body mass index (standardized)\nbp: average blood pressure (standardized)\ns1–s6: six serum measurements (standardized)\ndisease_progression (target): a quantitative measure of disease progression one year after baseline\n\n\nNote: Features are already standardized (mean ~0, variance ~1). This helps many models converge quickly and makes coefficients comparable in linear models.\n\n\n# Basic EDA\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\n\n\n\nScales: Here they are standardized, which simplifies model training.\nDistributions: Skewness/heavy tails can affect error metrics (e.g., MAE vs RMSE).\nTarget behavior: Understanding the spread of disease_progression will help interpret errors later.\n\n\n# Histograms for features and target\nimport matplotlib.pyplot as plt\n\n_ = df.hist(figsize=(12, 8), bins=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhy histograms? - Quick sense of distribution shape (normality, skewness, outliers). - Helps anticipate when certain models/metrics might struggle.\n\n# Scatter plots: individual features vs target\nimport matplotlib.pyplot as plt\n\nfeatures_to_plot = ['bmi', 'bp', 's5']\nfor col in features_to_plot:\n    plt.figure(figsize=(5,4))\n    plt.scatter(df[col], df['disease_progression'], alpha=0.7)\n    plt.xlabel(col)\n    plt.ylabel('disease_progression')\n    plt.title(f'{col} vs disease_progression')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading these plots - A monotonic trend (up or down) suggests a linear component. - Curvature suggests possible non-linear relationships (trees, interactions, neural nets may help). - Heteroscedasticity (variance changing with \\(x\\)) foreshadows issues for ordinary least squares assumptions."
  },
  {
    "objectID": "01_introduction_to_regression.html#what-comes-next",
    "href": "01_introduction_to_regression.html#what-comes-next",
    "title": "Introduction to Regression",
    "section": "",
    "text": "In the next notebook, we will fit a Linear Regression model on this dataset and discuss: - The linear model \\(\\hat{y} = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\\) - Ordinary Least Squares (OLS) objective and closed-form solution - Interpreting coefficients and assumptions - First set of evaluation metrics and diagnostic plots (moved fully to the following notebook)\nWe will then follow with a dedicated Model Evaluation notebook to build robust instincts for diagnosing bias/variance, under/overfitting, and when to consider regularization or non-linear models."
  },
  {
    "objectID": "01_introduction_to_regression.html#short-exercises-optional",
    "href": "01_introduction_to_regression.html#short-exercises-optional",
    "title": "Introduction to Regression",
    "section": "",
    "text": "Using the scatter plots above, which feature seems most correlated with disease_progression? Why?\nCompute the correlation matrix and identify the top-3 features most correlated with the target.\n(Think) What kinds of clinical or biological processes might explain non-linear patterns in this dataset? How would that influence your model choice?\n\n\n# (Optional) Correlation matrix to support Exercise 2\ncorr = df.corr(numeric_only=True)\ncorr['disease_progression'].sort_values(ascending=False)\n\ndisease_progression    1.000000\nbmi                    0.586450\ns5                     0.565883\nbp                     0.441482\ns4                     0.430453\ns6                     0.382483\ns1                     0.212022\nage                    0.187889\ns2                     0.174054\nsex                    0.043062\ns3                    -0.394789\nName: disease_progression, dtype: float64"
  },
  {
    "objectID": "00_setup_guide.html",
    "href": "00_setup_guide.html",
    "title": "Fundamentals of Regression",
    "section": "",
    "text": "Sign up for an NCI account if you don’t already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join."
  },
  {
    "objectID": "00_setup_guide.html#nci-account-setup",
    "href": "00_setup_guide.html#nci-account-setup",
    "title": "Fundamentals of Regression",
    "section": "",
    "text": "Sign up for an NCI account if you don’t already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join."
  },
  {
    "objectID": "00_setup_guide.html#nci-australian-research-environment-are",
    "href": "00_setup_guide.html#nci-australian-research-environment-are",
    "title": "Fundamentals of Regression",
    "section": "NCI Australian Research Environment (ARE)",
    "text": "NCI Australian Research Environment (ARE)\n\nConnect to NCI Australian Research Environment.\nBe sure you use your NCI ID (eg, ab1234) for the username and not your email address.\nUnder Featured Apps, find and click the JupterLab: Start a JupyterLab instance option. \nTo Launch a JuptyerLab session, set these resource requirements:\n\n\n\n\n\n\n\nResource\nValue\n\n\n\n\nWalltime (hours)\n5\n\n\nQueue\nnormalbw\n\n\nCompute Size\nsmall\n\n\nProject\ncd82\n\n\nStorage\nscratch/cd82\n\n\nAdvanced Options…\n\n\n\nModules\npython3/3.9.2\n\n\nPython or Conda virtual environment base\n/scratch/cd82/venv_workshop\n\n\n\nThen click the Launch button.\nThis will take you to your interactive session page you will see that that your JupyterLab session is Queued while ARE is searching for a compute node that will satisfy your requirements.\nOnce found, the page will update with a button that you can click to Open JupyterLab.\nHere is a screenshot of a JupyterLab landing page that should be similar to the one that opens in your web browser after starting the JupyterLab server on either macOS or Windows."
  },
  {
    "objectID": "00_setup_guide.html#transferring-workshop-notebooks",
    "href": "00_setup_guide.html#transferring-workshop-notebooks",
    "title": "Fundamentals of Regression",
    "section": "Transferring workshop notebooks",
    "text": "Transferring workshop notebooks\nWhen you have a Jupyter server running use JupyterLab file navigator to go the folder that has the same name as your username. Then make a new Jupyter notebook by clicking on the “Python 3” icon under “Notebook” section and run the following code in a cell:\n!rm -rf /scratch/cd82/$USER/notebooks\n!mkdir -p /scratch/cd82/$USER/notebooks\n!cp /scratch/cd82/regression_notebooks/* /scratch/cd82/$USER/notebooks/\n!ls /scratch/cd82/$USER/notebooks/\nAnd then use the Jupyter file browser to navigate to the directory: /scratch/cd82/$USER/notebooks/ (where $USER is your NCI username)"
  },
  {
    "objectID": "02_linear_regression.html",
    "href": "02_linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "In this lesson, we build a first predictive model on the Diabetes dataset using Ordinary Least Squares (OLS) linear regression. We will derive the objective, fit a model, inspect coefficients, and produce key diagnostic plots.\nLearning objectives - Write the linear model and OLS objective using proper notation. - Fit and evaluate a linear regression model on the Diabetes dataset. - Interpret coefficients and assess assumptions via residual diagnostics."
  },
  {
    "objectID": "02_linear_regression.html#linear-model-and-ols-objective",
    "href": "02_linear_regression.html#linear-model-and-ols-objective",
    "title": "Linear Regression",
    "section": "Linear Model and OLS Objective",
    "text": "Linear Model and OLS Objective\nA \\(p\\)-feature linear model predicts a continuous outcome as \\[\n\\hat{y} = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j \\,.\n\\]\nIn matrix form with \\(X \\in \\mathbb{R}^{n \\times p}\\), parameter vector \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}\\), and target \\(\\mathbf{y} \\in \\mathbb{R}^{n}\\), \\[\n\\hat{\\mathbf{y}} = X\\boldsymbol{\\beta} \\quad \\text{(assuming $X$ already includes a column of ones for the intercept).}\n\\]\nOrdinary Least Squares (OLS) estimates \\(\\boldsymbol{\\beta}\\) by minimizing the Mean Squared Error (MSE): \\[\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\lVert \\mathbf{y} - X\\boldsymbol{\\beta}\\rVert_2^2 \\,.\n\\]\nUnder standard conditions (full-rank \\(X\\)), the minimizer has a closed form: \\[\n\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X)^{-1}X^{\\top}\\mathbf{y} \\,.\n\\]\n\nIn practice, libraries use numerically stable solvers (e.g., QR decomposition) rather than forming \\((X^{\\top}X)^{-1}\\) explicitly."
  },
  {
    "objectID": "02_linear_regression.html#key-assumptions-diagnostics-later",
    "href": "02_linear_regression.html#key-assumptions-diagnostics-later",
    "title": "Linear Regression",
    "section": "Key Assumptions (Diagnostics Later)",
    "text": "Key Assumptions (Diagnostics Later)\n\nLinearity: The expected value of \\(y\\) is a linear combination of features.\nIndependence: Errors are independent across observations.\nHomoscedasticity: Constant variance of errors across fitted values.\nNormality of residuals: Residuals are approximately Gaussian (helps inference, less critical for prediction quality).\n\nWe will visualize residuals vs. predictions and the distribution of residuals to get a feel for these assumptions.\n\n# Load Diabetes dataset and fit a Linear Regression model\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport pandas as pd\nimport numpy as np\n\n\n# Load dataset (already standardized features)\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n178.0\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018114\n0.044485\n104.0\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017293\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046883\n0.015491\n132.0\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044529\n-0.025930\n220.0\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081413\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004222\n0.003064\n57.0\n\n\n\n\n442 rows × 11 columns\n\n\n\n\nX = df.drop(columns=['disease_progression'])\ny = df['disease_progression']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\n# Predictions\ny_pred_train = linreg.predict(X_train)\ny_pred_test = linreg.predict(X_test)\nprint(y_pred_test[:10])\nprint(y_test[:10].values)\n\n[139.5475584  179.51720835 134.03875572 291.41702925 123.78965872\n  92.1723465  258.23238899 181.33732057  90.22411311 108.63375858]\n[219.  70. 202. 230. 111.  84. 242. 272.  94.  96.]\n\n\n\n# Metrics\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test  = mean_squared_error(y_test, y_pred_test)\nrmse_train = np.sqrt(mse_train)\nrmse_test  = np.sqrt(mse_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test  = mean_absolute_error(y_test, y_pred_test)\nr2_train = r2_score(y_train, y_pred_train)\nr2_test  = r2_score(y_test, y_pred_test)\n\nmetrics_df = pd.DataFrame({\n    'set': ['train', 'test'],\n    'MSE': [mse_train, mse_test],\n    'RMSE': [rmse_train, rmse_test],\n    'MAE': [mae_train, mae_test],\n    'R2': [r2_train, r2_test],\n})\nmetrics_df\n\n\n\n\n\n\n\n\nset\nMSE\nRMSE\nMAE\nR2\n\n\n\n\n0\ntrain\n2868.549703\n53.558843\n43.483504\n0.527919\n\n\n1\ntest\n2900.193628\n53.853446\n42.794095\n0.452603\n\n\n\n\n\n\n\n\nQuick Reading of Metrics\n\nRMSE and MAE are in the same units as the target (disease_progression), making them more interpretable than MSE.\n\\(R^2\\) shows the proportion of variance explained by the model. Values closer to \\(1\\) are better. Negative \\(R^2\\) on the test set indicates severe mismatch (worse than predicting the mean).\n\nCompare train vs test: a large gap often signals overfitting; similar but poor scores can signal underfitting or model misspecification.\n\n# Inspect learned coefficients\nimport pandas as pd\ncoef_df = pd.DataFrame({\n    'feature': X.columns,\n    'coefficient': linreg.coef_\n}).sort_values(by='coefficient', key=abs, ascending=False).reset_index(drop=True)\n\nintercept = linreg.intercept_\ndisplay(coef_df)\nprint(\"Intercept (beta_0):\", intercept)\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n0\ns1\n-931.488846\n\n\n1\ns5\n736.198859\n\n\n2\nbmi\n542.428759\n\n\n3\ns2\n518.062277\n\n\n4\nbp\n347.703844\n\n\n5\ns4\n275.317902\n\n\n6\nsex\n-241.964362\n\n\n7\ns3\n163.419983\n\n\n8\ns6\n48.670657\n\n\n9\nage\n37.904021\n\n\n\n\n\n\n\nIntercept (beta_0): 151.34560453985995\n\n\nInterpreting coefficients (with standardized features) - A positive coefficient for feature \\(x_j\\) means that increasing \\(x_j\\) (holding others fixed) tends to increase predicted \\(y\\). - A negative coefficient means the opposite. - Because the Diabetes features are standardized, coefficients are roughly comparable in magnitude.\n\nInterpretation is conditional on other features in the model; correlated features can complicate attribution."
  },
  {
    "objectID": "02_linear_regression.html#optional-statsmodels-ols-for-statistical-summary",
    "href": "02_linear_regression.html#optional-statsmodels-ols-for-statistical-summary",
    "title": "Linear Regression",
    "section": "Optional: Statsmodels OLS for Statistical Summary",
    "text": "Optional: Statsmodels OLS for Statistical Summary\nWhile scikit-learn focuses on prediction, the statsmodels library provides detailed statistical inference tools.\nUsing statsmodels.OLS, we can obtain: - Standard errors and confidence intervals for coefficients - p-values for hypothesis tests - Model fit statistics like AIC and BIC\nThis is useful when interpreting the model in a traditional statistics context.\n\nimport statsmodels.api as sm\n\n# Add constant for intercept\nX_train_const = sm.add_constant(X_train)\nols_model = sm.OLS(y_train, X_train_const).fit()\n\n# Show a rich statistical summary\nprint(ols_model.summary())\n\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     disease_progression   R-squared:                       0.528\nModel:                             OLS   Adj. R-squared:                  0.514\nMethod:                  Least Squares   F-statistic:                     38.25\nDate:                 Tue, 09 Sep 2025   Prob (F-statistic):           5.41e-50\nTime:                         13:22:36   Log-Likelihood:                -1906.1\nNo. Observations:                  353   AIC:                             3834.\nDf Residuals:                      342   BIC:                             3877.\nDf Model:                           10                                         \nCovariance Type:             nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        151.3456      2.902     52.155      0.000     145.638     157.053\nage           37.9040     69.056      0.549      0.583     -97.923     173.731\nsex         -241.9644     68.570     -3.529      0.000    -376.836    -107.093\nbmi          542.4288     76.956      7.049      0.000     391.062     693.795\nbp           347.7038     71.357      4.873      0.000     207.350     488.057\ns1          -931.4888    451.138     -2.065      0.040   -1818.844     -44.134\ns2           518.0623    364.114      1.423      0.156    -198.122    1234.247\ns3           163.4200    233.014      0.701      0.484    -294.901     621.741\ns4           275.3179    185.400      1.485      0.138     -89.349     639.985\ns5           736.1989    192.437      3.826      0.000     357.689    1114.709\ns6            48.6707     73.435      0.663      0.508     -95.771     193.113\n==============================================================================\nOmnibus:                        1.457   Durbin-Watson:                   1.794\nProb(Omnibus):                  0.483   Jarque-Bera (JB):                1.412\nSkew:                           0.064   Prob(JB):                        0.494\nKurtosis:                       2.718   Cond. No.                         219.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Predicted vs Actual (Test set)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(5,5))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\n# 45-degree reference line\nmin_val = min(y_test.min(), y_pred_test.min())\nmax_val = max(y_test.max(), y_pred_test.max())\nplt.plot([min_val, max_val], [min_val, max_val])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Predicted vs Actual (Test)\")\nplt.show()\n\n\n\n\n\n\n\n\nReading the plot - Points along the diagonal indicate accurate predictions. - Systematic curvature away from the line suggests model misspecification (e.g., non-linearities). - Wide vertical spread indicates high variance (errors) for certain ranges of the target.\n\n# Residual diagnostics (Test set)\nimport matplotlib.pyplot as plt\nimport numpy as np\nresiduals = y_test - y_pred_test\n\n# Residuals vs Predicted\nplt.figure(figsize=(5,4))\nplt.scatter(y_pred_test, residuals, alpha=0.7)\nplt.axhline(0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals (y - y_hat)\")\nplt.title(\"Residuals vs Predicted (Test)\")\nplt.show()\n\n# Histogram of residuals\nplt.figure(figsize=(5,4))\nplt.hist(residuals, bins=20)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Residuals Distribution (Test)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics - Homoscedasticity: Look for a constant vertical spread across predictions. A funnel shape suggests heteroscedasticity. - Non-linearity: Curved patterns indicate linearity violations. - Normality: A roughly symmetric, bell-shaped residual histogram is supportive (though not strictly required for good predictions).\n\n# Optional: Cross-validated R^2 to gauge robustness\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ncv_scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=5)\ncv_scores, np.mean(cv_scores), np.std(cv_scores)\n\n(array([0.42955615, 0.52259939, 0.48268054, 0.42649776, 0.55024834]),\n np.float64(0.4823164359086422),\n np.float64(0.0492685775119038))\n\n\nCross-validation provides a more stable estimate of performance by averaging across folds. A high standard deviation indicates sensitivity to how the data is split."
  },
  {
    "objectID": "02_linear_regression.html#summary",
    "href": "02_linear_regression.html#summary",
    "title": "Linear Regression",
    "section": "Summary",
    "text": "Summary\n\nWe fit a linear regression model and examined coefficients and diagnostics.\nWe computed RMSE, MAE, and \\(R^2\\) on train/test splits.\nResidual plots help check assumptions and guide next steps.\n\nNext: A dedicated Model Evaluation notebook will deepen our understanding with additional metrics, visual checks, and learning curves before we progress to regularization (Ridge, Lasso) to combat potential overfitting and multicollinearity."
  },
  {
    "objectID": "02_linear_regression.html#exercises",
    "href": "02_linear_regression.html#exercises",
    "title": "Linear Regression",
    "section": "Exercises",
    "text": "Exercises\n\nFeature selection trial: Retrain the model using only the top-5 features by absolute coefficient magnitude. Compare test RMSE and \\(R^2\\).\nPolynomial feature experiment: Create a squared term for bmi and refit the model. Did residual patterns improve?"
  },
  {
    "objectID": "04_regularization.html",
    "href": "04_regularization.html",
    "title": "Regularization: Ridge and Lasso",
    "section": "",
    "text": "In this lesson we address overfitting and multicollinearity using Ridge (\\(L_2\\)) and Lasso (\\(L_1\\)) regression. We will compare their effects on coefficients, tune the regularization strength with cross‑validation, and discuss when each is appropriate.\nLearning objectives - Understand how \\(L_2\\) and \\(L_1\\) penalties modify the OLS objective. - Train and tune Ridge and Lasso with cross‑validation. - Interpret coefficient shrinkage and feature selection effects. - Read validation curves and coefficient paths as \\(\\alpha\\) varies."
  },
  {
    "objectID": "04_regularization.html#why-regularize",
    "href": "04_regularization.html#why-regularize",
    "title": "Regularization: Ridge and Lasso",
    "section": "Why Regularize?",
    "text": "Why Regularize?\nOLS minimizes \\[\n\\mathcal{L}_{\\mathrm{OLS}}(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\,,\n\\quad \\text{with } \\hat{y}_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij}.\n\\]\nWhen features are correlated or \\(p\\) is large relative to \\(n\\), OLS can have high variance. Regularization adds a penalty on coefficient size to reduce variance at the cost of bias.\n\nRidge (\\(L_2\\)) Regression\n\\[\n\\mathcal{L}_{\\mathrm{Ridge}}(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^p \\beta_j^2\\,.\n\\] - Shrinks coefficients continuously toward zero. - Never sets coefficients exactly to zero. - Good for multicollinearity.\n\n\nLasso (\\(L_1\\)) Regression\n\\[\n\\mathcal{L}_{\\mathrm{Lasso}}(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^p \\lvert \\beta_j \\rvert \\,.\n\\] - Encourages sparsity: can set some coefficients exactly to zero. - Useful for feature selection.\nThe regularization strength \\(\\\\alpha \\\\ge 0\\) controls shrinkage; larger \\(\\\\alpha\\) means stronger regularization.\n\n# Data setup: Diabetes dataset\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nX = df.drop(columns=['disease_progression'])\ny = df['disease_progression']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\ndf.head()\n\nTrain shape: (353, 10)  Test shape: (89, 10)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0"
  },
  {
    "objectID": "04_regularization.html#standardization-pipelines",
    "href": "04_regularization.html#standardization-pipelines",
    "title": "Regularization: Ridge and Lasso",
    "section": "Standardization & Pipelines",
    "text": "Standardization & Pipelines\nRegularized linear models are sensitive to feature scaling because the penalty depends on coefficient magnitude.\nAlthough the Diabetes features are already standardized, we demonstrate a Pipeline with StandardScaler to encourage good practice (especially for future datasets).\n\n# Ridge & Lasso with cross-validation and safe parallelization for HPC (NCI ARE)\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n# Define CV and alpha grid (log-spaced)\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nalphas = np.logspace(-3, 3, 20)\n\nridge_pipeline = Pipeline([('scaler', StandardScaler()), ('model', Ridge())])\nlasso_pipeline = Pipeline([('scaler', StandardScaler()), ('model', Lasso(max_iter=10000))])\n\ndef cv_scores_for_alphas(pipeline, alphas, X, y):\n    r2_means, rmse_means = [], []\n    for a in alphas:\n        model = Pipeline(pipeline.steps[:-1] + [('model', pipeline.named_steps['model'].__class__(alpha=a, max_iter=10000) if pipeline.named_steps['model'].__class__ is Lasso else pipeline.named_steps['model'].__class__(alpha=a))])\n        # Prefer threading backend to avoid _posixsubprocess issues on some platforms\n        try:\n            from joblib import parallel_backend\n            with parallel_backend('threading'):\n                r2 = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1)\n                neg_mse = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n        except Exception:\n            r2 = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=1)\n            neg_mse = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=1)\n        r2_means.append(np.mean(r2))\n        rmse_means.append(np.sqrt(-np.mean(neg_mse)))\n    return np.array(r2_means), np.array(rmse_means)\n\nridge_r2, ridge_rmse = cv_scores_for_alphas(ridge_pipeline, alphas, X, y)\nlasso_r2, lasso_rmse = cv_scores_for_alphas(lasso_pipeline, alphas, X, y)\n\nbest_ridge_idx = np.argmax(ridge_r2)\nbest_lasso_idx = np.argmax(lasso_r2)\n\nbest_ridge_alpha = alphas[best_ridge_idx]\nbest_lasso_alpha = alphas[best_lasso_idx]\n\nbest_ridge_alpha, best_lasso_alpha\n\n(np.float64(26.366508987303554), np.float64(0.6951927961775606))\n\n\nReading CV curves - Choose the \\(\\\\alpha\\) that maximizes validation \\(R^2\\) (or minimizes RMSE). - Ridge curves are usually smoother; Lasso may plateau and then drop as many coefficients are zeroed out.\n\n# Plot validation curves for Ridge and Lasso\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(6,4))\nplt.semilogx(alphas, ridge_r2, marker='o', label='Ridge CV R^2')\nplt.semilogx(alphas, lasso_r2, marker='s', label='Lasso CV R^2')\nplt.xlabel(\"alpha\")\nplt.ylabel(\"R^2 (CV mean)\")\nplt.title(\"Validation Curves\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.semilogx(alphas, ridge_rmse, marker='o', label='Ridge CV RMSE')\nplt.semilogx(alphas, lasso_rmse, marker='s', label='Lasso CV RMSE')\nplt.xlabel(\"alpha\")\nplt.ylabel(\"RMSE (CV mean)\")\nplt.title(\"Validation Curves (RMSE)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Fit best Ridge and Lasso and evaluate on the test set\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nimport numpy as np\n\nbest_ridge = Pipeline([('scaler', StandardScaler()), ('model', Ridge(alpha=best_ridge_alpha))])\nbest_lasso = Pipeline([('scaler', StandardScaler()), ('model', Lasso(alpha=best_lasso_alpha, max_iter=10000))])\n\nbest_ridge.fit(X_train, y_train)\nbest_lasso.fit(X_train, y_train)\n\nfor name, model in [('Ridge', best_ridge), ('Lasso', best_lasso)]:\n    y_pred = model.predict(X_test)\n    r2  = r2_score(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae  = mean_absolute_error(y_test, y_pred)\n    print(f\"{name} (alpha={model.named_steps['model'].alpha:.4f}) -&gt; Test R^2: {r2:.3f}, RMSE: {rmse:.3f}, MAE: {mae:.3f}\")\n\nRidge (alpha=26.3665) -&gt; Test R^2: 0.459, RMSE: 53.518, MAE: 42.922\nLasso (alpha=0.6952) -&gt; Test R^2: 0.463, RMSE: 53.324, MAE: 42.825"
  },
  {
    "objectID": "04_regularization.html#coefficient-paths",
    "href": "04_regularization.html#coefficient-paths",
    "title": "Regularization: Ridge and Lasso",
    "section": "Coefficient Paths",
    "text": "Coefficient Paths\nBy tracking coefficients across a grid of \\(\\\\alpha\\) values, we can visualize shrinkage (Ridge) and sparsity (Lasso).\n\n# Coefficient paths for Ridge and Lasso\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nfeature_names = X.columns\n\n# Ridge path\nridge_coefs = []\nlasso_coefs = []\nfor a in alphas:\n    Ridge(alpha=a).fit(X_scaled, y)\n    ridge_coefs.append(Ridge(alpha=a).fit(X_scaled, y).coef_)\n    Lasso(alpha=a, max_iter=10000).fit(X_scaled, y)\n    lasso_coefs.append(Lasso(alpha=a, max_iter=10000).fit(X_scaled, y).coef_)\n\nridge_coefs = np.array(ridge_coefs)\nlasso_coefs = np.array(lasso_coefs)\n\nplt.figure(figsize=(7,5))\nfor j in range(len(feature_names)):\n    plt.semilogx(alphas, ridge_coefs[:, j])\nplt.xlabel(\"alpha\")\nplt.ylabel(\"Coefficient value\")\nplt.title(\"Ridge Coefficient Paths\")\nplt.show()\n\nplt.figure(figsize=(7,5))\nfor j in range(len(feature_names)):\n    plt.semilogx(alphas, lasso_coefs[:, j])\nplt.xlabel(\"alpha\")\nplt.ylabel(\"Coefficient value\")\nplt.title(\"Lasso Coefficient Paths\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting paths - Ridge: all coefficients shrink smoothly toward \\(0\\) as \\(\\\\alpha\\) increases. - Lasso: coefficients hit exactly \\(0\\) at different \\(\\\\alpha\\) thresholds \\(\\\\Rightarrow\\) implicit feature selection."
  },
  {
    "objectID": "04_regularization.html#choosing-between-ridge-and-lasso",
    "href": "04_regularization.html#choosing-between-ridge-and-lasso",
    "title": "Regularization: Ridge and Lasso",
    "section": "Choosing Between Ridge and Lasso",
    "text": "Choosing Between Ridge and Lasso\n\nUse Ridge when many features have small/medium effects and multicollinearity is present.\nUse Lasso when you expect only a subset of features to be relevant and want sparse models.\nElastic Net (not covered here) blends \\(L_1\\) and \\(L_2\\) and can be a good default when unsure."
  },
  {
    "objectID": "04_regularization.html#parallelisation-on-nci-are-important",
    "href": "04_regularization.html#parallelisation-on-nci-are-important",
    "title": "Regularization: Ridge and Lasso",
    "section": "Parallelisation on NCI ARE (Important)",
    "text": "Parallelisation on NCI ARE (Important)\nOn some HPC Jupyter environments, n_jobs=-1 with the default backend may raise No module named '_posixsubprocess'.\nWrap CV with the threading backend as shown in this notebook, or set n_jobs=1 for compatibility.\nfrom joblib import parallel_backend\nwith parallel_backend('threading'):\n    # your cross_val_score(...) calls with n_jobs=-1\n    pass"
  },
  {
    "objectID": "04_regularization.html#exercises",
    "href": "04_regularization.html#exercises",
    "title": "Regularization: Ridge and Lasso",
    "section": "Exercises",
    "text": "Exercises\n\nOne‑SE rule: Using the CV curves, pick the largest \\(\\\\alpha\\) within one standard error of the best \\(R^2\\) (requires storing fold‑level scores). Does it generalize better?\nSparsity check (Lasso): For the chosen \\(\\\\alpha\\), how many coefficients are exactly zero? Which features remain?\nTarget transform: Try predicting \\(\\\\log(1+y)\\) instead of \\(y\\). Does either model improve on \\(R^2\\)?\nStability: Shuffle random_state in K‑fold and report the variance in the selected \\(\\\\alpha\\) and test \\(R^2\\)."
  },
  {
    "objectID": "06_neural_network.html",
    "href": "06_neural_network.html",
    "title": "Neural Network Regression (Diabetes)",
    "section": "",
    "text": "In this lesson we build a neural network regressor (MLP) for the Diabetes dataset. We’ll handle mixed data types (numeric + categorical) via preprocessing pipelines, train an MLPRegressor, and evaluate with metrics and diagnostics. We’ll also add a lightweight uncertainty estimate using an ensemble of MLPs.\nLearning objectives - Preprocess mixed features with ColumnTransformer (scaling + standardization). - Train and tune an MLPRegressor with early stopping. - Evaluate with \\(\\text{RMSE}\\), \\(\\text{MAE}\\), and \\(R^2\\); create residual and calibration plots. - Use a small ensemble of MLPs for predictive mean and uncertainty (std)."
  },
  {
    "objectID": "06_neural_network.html#mlp-for-regression-theory",
    "href": "06_neural_network.html#mlp-for-regression-theory",
    "title": "Neural Network Regression (Diabetes)",
    "section": "MLP for Regression (Theory)",
    "text": "MLP for Regression (Theory)\nA feed‑forward network with \\(L\\) layers composes affine maps and nonlinearities: \\[\n\\hat{y} = f(x;\\`theta) = W^{(L)} a^{(L-1)} + b^{(L)}, \\quad a^{(\\ell)} = \\phi\\!\\left(W^{(\\ell)} a^{(\\ell-1)} + b^{(\\ell)}\\right),\n\\] with \\(a^{(0)} = x\\), parameters \\(\\theta = \\{W^{(\\ell)}, b^{(\\ell)}\\}\\), and activation \\(\\phi\\) (e.g., ReLU).\nWe minimize mean squared error (MSE): \\[\n\\mathcal{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i;\\theta))^2.\n\\]\n\nEarly stopping regularizes by halting training if validation loss stops improving.\nStandardization of numeric features helps optimization (well‑scaled inputs).\nOne‑hot encoding for categorical variables lets the network learn from categories."
  },
  {
    "objectID": "06_neural_network.html#data-diabetes-sklearn",
    "href": "06_neural_network.html#data-diabetes-sklearn",
    "title": "Neural Network Regression (Diabetes)",
    "section": "Data: Diabetes (sklearn)",
    "text": "Data: Diabetes (sklearn)\nWe use the built-in Diabetes dataset from scikit-learn, which contains 10 standardized numeric predictors and a continuous target disease_progression (a quantitative measure one year after baseline).\n\nNo categorical features here — preprocessing only needs standardization.\n\n\n# Load Diabetes dataset (built-in)\nfrom sklearn.datasets import load_diabetes\nimport pandas as pd\n\ndata = load_diabetes(as_frame=True)\ndf = data.frame.copy()\ndf.rename(columns={'target': 'disease_progression'}, inplace=True)\n\nprint('Loaded Diabetes dataset. Shape:', df.shape)\ndf.head()\n\nLoaded Diabetes dataset. Shape: (442, 11)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\n# Split and build preprocessing pipeline (numeric-only)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\n\ntarget = 'disease_progression'\ny = df[target].values\nX = df.drop(columns=[target])\n\nnumeric_features = X.columns.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(with_mean=True, with_std=True), numeric_features),\n    ],\n    remainder='drop'\n)\n\nnumeric_features\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\n# Baseline MLPRegressor with early stopping\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\nmlp = MLPRegressor(\n    hidden_layer_sizes=(64, 32),\n    activation='relu',\n    solver='adam',\n    learning_rate_init=1e-3,\n    alpha=1e-4,                # L2 penalty\n    max_iter=500,\n    early_stopping=True,\n    n_iter_no_change=20,\n    validation_fraction=0.15,\n    random_state=42\n)\n\nmodel = Pipeline([('prep', preprocess), ('mlp', mlp)])\nmodel.fit(X_train, y_train)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test  = np.sqrt(mean_squared_error(y_test, y_pred_test))\nmae_train  = mean_absolute_error(y_train, y_pred_train)\nmae_test   = mean_absolute_error(y_test, y_pred_test)\nr2_train   = r2_score(y_train, y_pred_train)\nr2_test    = r2_score(y_test, y_pred_test)\n\nprint(f\"Train: RMSE={rmse_train:.1f}  MAE={mae_train:.1f}  R^2={r2_train:.3f}\")\nprint(f\"Test : RMSE={rmse_test:.1f}   MAE={mae_test:.1f}   R^2={r2_test:.3f}\")\n\nTrain: RMSE=55.2  MAE=43.3  R^2=0.498\nTest : RMSE=57.2   MAE=46.7   R^2=0.382\n\n\n\n# Diagnostics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nresiduals = y_test - y_pred_test\n\n# Predicted vs Actual\nplt.figure(figsize=(5,5))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nlo = min(y_test.min(), y_pred_test.min())\nhi = max(y_test.max(), y_pred_test.max())\nplt.plot([lo, hi], [lo, hi])\nplt.xlabel(\"Actual disease_progression\")\nplt.ylabel(\"Predicted disease_progression\")\nplt.title(\"Predicted vs Actual (Test)\")\nplt.show()\n\n# Residuals vs Predicted\nplt.figure(figsize=(6,4))\nplt.scatter(y_pred_test, residuals, alpha=0.7)\nplt.axhline(0)\nplt.xlabel(\"Predicted disease_progression\")\nplt.ylabel(\"Residual (y - ŷ)\")\nplt.title(\"Residuals vs Predicted (Test)\")\nplt.show()\n\n# Residual histogram\nplt.figure(figsize=(6,4))\nplt.hist(residuals, bins=25)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Residuals Distribution (Test)\")\nplt.show()"
  },
  {
    "objectID": "06_neural_network.html#lightweight-uncertainty-via-ensembling",
    "href": "06_neural_network.html#lightweight-uncertainty-via-ensembling",
    "title": "Neural Network Regression (Diabetes)",
    "section": "Lightweight Uncertainty via Ensembling",
    "text": "Lightweight Uncertainty via Ensembling\nTrain \\(K\\) networks with different random seeds and average their predictions: - The ensemble mean reduces variance. - The ensemble standard deviation (per sample) provides a pragmatic uncertainty proxy.\n\n# Train a small ensemble and compute mean/std of predictions\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nK = 5\npreds = []\n\nfor seed in range(K):\n    mlp_k = MLPRegressor(\n        hidden_layer_sizes=(64, 32),\n        activation='relu',\n        solver='adam',\n        learning_rate_init=1e-3,\n        alpha=1e-4,\n        max_iter=500,\n        early_stopping=True,\n        n_iter_no_change=15,\n        validation_fraction=0.15,\n        random_state=seed\n    )\n    model_k = Pipeline([('prep', preprocess), ('mlp', mlp_k)])\n    model_k.fit(X_train, y_train)\n    preds.append(model_k.predict(X_test))\n\npreds = np.vstack(preds)              # shape (K, n_test)\ny_pred_mean = preds.mean(axis=0)      # ensemble mean\ny_pred_std  = preds.std(axis=0)       # ensemble std (uncertainty proxy)\n\n# Compare single vs ensemble\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nrmse_ens = np.sqrt(mean_squared_error(y_test, y_pred_mean))\nmae_ens  = mean_absolute_error(y_test, y_pred_mean)\nr2_ens   = r2_score(y_test, y_pred_mean)\n\nprint(f\"Ensemble: RMSE={rmse_ens:.1f}  MAE={mae_ens:.1f}  R^2={r2_ens:.3f}\")\nprint(\"Per-sample prediction std (first 10):\", np.round(y_pred_std[:10], 1))\n\nEnsemble: RMSE=54.4  MAE=44.1  R^2=0.442\nPer-sample prediction std (first 10): [ 9.7  3.5  9.4  8.2  7.4  4.  10.   6.9  8.8 10.5]\n\n\n\n# Visualize uncertainty: error vs ensemble std\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nabs_err = np.abs(y_test - y_pred_mean)\n\nplt.figure(figsize=(6,4))\nplt.scatter(y_pred_std, abs_err, alpha=0.6)\nplt.xlabel(\"Ensemble prediction std (uncertainty)\")\nplt.ylabel(\"Absolute error |y - ŷ|\")\nplt.title(\"Uncertainty proxy vs Absolute Error (Test)\")\nplt.show()\n\n# Calibration-style plot: group by uncertainty quantiles\nq = np.quantile(y_pred_std, [0, 0.25, 0.5, 0.75, 1.0])\nbins = np.digitize(y_pred_std, q[1:-1], right=True)\ngroup_mae = [abs_err[bins==i].mean() if np.any(bins==i) else np.nan for i in range(4)]\nplt.figure(figsize=(6,4))\nplt.bar([\"Q1\",\"Q2\",\"Q3\",\"Q4\"], group_mae)\nplt.ylabel(\"Mean |error|\")\nplt.title(\"Errors by Uncertainty Quartile (lower→higher)\")\nplt.show()"
  },
  {
    "objectID": "06_neural_network.html#parallelisation-on-nci-are",
    "href": "06_neural_network.html#parallelisation-on-nci-are",
    "title": "Neural Network Regression (Diabetes)",
    "section": "Parallelisation on NCI ARE",
    "text": "Parallelisation on NCI ARE\n\nMLPRegressor itself does not expose n_jobs, but you can parallelise ensembles with joblib or the threading backend:\n\nfrom joblib import Parallel, delayed, parallel_backend\n\nwith parallel_backend('threading'):\n    preds = Parallel(n_jobs=-1)(\n        delayed(Pipeline([('prep', preprocess), ('mlp', MLPRegressor(..., random_state=seed))]).fit(X_train, y_train).predict)(X_test)\n        for seed in range(K)\n    )\n\nAvoid oversubscription (too many threads) on shared nodes; start with n_jobs=4 and increase if the node has capacity."
  },
  {
    "objectID": "06_neural_network.html#exercises",
    "href": "06_neural_network.html#exercises",
    "title": "Neural Network Regression (Diabetes)",
    "section": "Exercises",
    "text": "Exercises\n\nHyperparameters: Try hidden_layer_sizes=(128, 64) and alpha=1e-3. Does \\(R^2\\) improve on test?\nInput transforms: Add polynomial features for bmi (or interactions) before the MLP. Any gain?\nUncertainty: Increase the ensemble size \\(K\\) to 10–20 (time permitting). Does the std correlate better with errors?\nRobustness: Train with smoker removed. How do performance and uncertainty change?"
  }
]