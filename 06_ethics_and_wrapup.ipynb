{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible Use, Bias, and Academic Writing\n",
        "\n",
        "This final notebook is fully theoretical and serves as the reflective conclusion of the LLM in Research workshop.  \n",
        "By now, participants have experimented with literature summarisation, retrieval-augmented generation (RAG), and code generation.  \n",
        "Here, we step back and discuss **how to use these capabilities responsibly** in academic and research settings.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Why Ethics Matter in AI Research\n",
        "\n",
        "Large Language Models (LLMs) are not merely tools ‚Äî they shape how research is **written, cited, and communicated**.  \n",
        "Ethical use ensures that our work remains *reproducible*, *transparent*, and *trustworthy*.\n",
        "\n",
        "- **Reproducibility:** others should be able to verify what the model produced.\n",
        "- **Transparency:** readers must know when AI was used.\n",
        "- **Accountability:** responsibility stays with the human researcher, not the AI.\n",
        "\n",
        "Even small ethical lapses ‚Äî unverified summaries, missing citations, or hidden AI contributions ‚Äî can propagate misinformation through the scholarly record.\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ Hallucination and Fabrication\n",
        "\n",
        "**Hallucination** means that an LLM produces text that *sounds plausible but is not factually true*.  \n",
        "It happens because the model predicts *likely words*, not *verified facts*.\n",
        "\n",
        "### Common manifestations:\n",
        "- Invented citations (‚ÄúSmith et al., 2017‚Äù that never existed)\n",
        "- Confident but false claims (‚ÄúThis study showed quantum teleportation of viruses‚Äù)\n",
        "- Misquoted statistics or data sources\n",
        "\n",
        "### Why it happens:\n",
        "- The model has no built-in access to real-time databases.\n",
        "- It interpolates between known facts to fill gaps.\n",
        "- Sampling randomness (temperature) can increase hallucination frequency.\n",
        "\n",
        "### Mitigation strategies:\n",
        "- Cross-check every factual claim.\n",
        "- Ask for sources or DOIs explicitly.\n",
        "- Use RAG workflows (as in Notebook 04) to ground responses in retrieved evidence.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Bias in Language Models\n",
        "\n",
        "LLMs learn from large-scale human text corpora and **inherit human biases** ‚Äî social, geographic, institutional, and ideological.\n",
        "\n",
        "### Types of bias\n",
        "| Type | Example | Possible Consequence |\n",
        "|------|----------|----------------------|\n",
        "| Gender bias | \"The nurse‚Ä¶ she\"; \"The engineer‚Ä¶ he\" | Reinforces stereotypes |\n",
        "| Geographic bias | U.S.-centric examples | Marginalises local or Global South contexts |\n",
        "| Institutional bias | Over-representation of elite universities | Skews perception of authority |\n",
        "| Topical bias | Focus on trendy disciplines | Neglects underrepresented fields |\n",
        "\n",
        "### Reflection prompts\n",
        "- Does your prompt assume one kind of author, culture, or region?\n",
        "- Are the model‚Äôs suggestions diverse and representative?\n",
        "- How could biased phrasing influence downstream analyses?\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Academic Writing and Integrity\n",
        "\n",
        "LLMs can help researchers:\n",
        "- Rephrase complex sentences\n",
        "- Improve grammar and flow\n",
        "- Suggest structure or headings\n",
        "\n",
        "However, there are **boundaries of ethical use**.\n",
        "\n",
        "| Practice | Example | Safe? | Comment |\n",
        "|-----------|----------|--------|----------|\n",
        "| Paraphrasing your own draft for clarity | ‚ÄúRewrite this paragraph in academic tone.‚Äù | ‚úÖ | Keep authorship transparent. |\n",
        "| Summarising literature you provide | ‚ÄúSummarise this abstract in 3 sentences.‚Äù | ‚úÖ | Verify facts manually. |\n",
        "| Generating original citations | ‚ÄúList 5 references on‚Ä¶‚Äù | ‚ùå | May produce hallucinated papers. |\n",
        "| Copying large model outputs into a manuscript | ‚Äì | ‚ö†Ô∏è | Requires attribution (‚ÄúAs generated by an LLM‚Ä¶‚Äù). |\n",
        "\n",
        "**Rule of thumb:**  \n",
        "> AI tools can assist writing but cannot author scholarship.\n",
        "\n",
        "Always acknowledge LLM assistance in the acknowledgements or methods section.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Reproducibility and Documentation\n",
        "\n",
        "A reproducible AI workflow records **exact conditions** under which results were produced.\n",
        "\n",
        "### Good documentation includes:\n",
        "- Model name and version (e.g., *Llama-3.1-70B*, Groq API, May 2025)\n",
        "- Prompt text and parameters (temperature, max tokens)\n",
        "- Retrieval corpus and date of access\n",
        "- API source or endpoint\n",
        "- Any local preprocessing code or filters\n",
        "\n",
        "### Example documentation block:\n",
        "> ‚ÄúSummaries were generated using Groq‚Äôs Llama-3.1-70B model (API version 2025-05-02) with temperature=0.2.  \n",
        "> Input abstracts were retrieved from ArXiv on 2025-05-10 using the query ‚Äòquantum batteries‚Äô.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üìú Ethical Checklist for LLM Use\n",
        "\n",
        "| Aspect | Example | Safe Usage |\n",
        "|--------|----------|------------|\n",
        "| Citation | ‚ÄúAs summarised by LLM, based on ArXiv data‚Äù | ‚úÖ |\n",
        "| Fabrication | ‚ÄúThis study found‚Ä¶‚Äù (invented) | ‚ùå |\n",
        "| Sensitive data | Upload of patient or private data | ‚ùå |\n",
        "| Attribution | ‚ÄúGenerated draft reviewed by author‚Äù | ‚úÖ |\n",
        "| Confidentiality | Submitting unpublished manuscripts | ‚ö†Ô∏è |\n",
        "| Transparency | ‚ÄúEdited for clarity using an LLM tool‚Äù | ‚úÖ |\n",
        "\n",
        "### Institutional Guidelines\n",
        "- Many universities now classify unacknowledged AI writing as **academic misconduct**.\n",
        "- Check your organisation‚Äôs policies (often listed under *Research Integrity* or *Publication Ethics*).\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Final Reflection\n",
        "\n",
        "- What tasks will you responsibly delegate to an LLM?  \n",
        "- How can you make your use of AI **transparent and auditable**?  \n",
        "- Would your workflow remain valid if the model were updated or replaced?\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "1. **LLMs are probabilistic assistants, not authorities.**  \n",
        "2. **Bias and hallucination are systemic, not accidental.**  \n",
        "3. **Ethical use means attribution, verification, and documentation.**  \n",
        "4. **Transparency builds trust ‚Äî in both research and AI.**\n",
        "\n",
        "> ‚ÄúScience advances through transparency, not shortcuts.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### End of Workshop\n",
        "\n",
        "Congratulations on completing the *LLMs in Research* series!  \n",
        "You now understand not only how to use LLMs for research but also how to do so **responsibly and reproducibly**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
